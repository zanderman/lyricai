{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd064d019df8f386dcc423f9fd084afce6ddb1bf16f9fb1fd7275a3007e4feb955b",
   "display_name": "Python 3.9.2 64-bit ('lyricai-GYvo4rtv': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "source": [
    "## Load lyric files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_files(root_dir: str) -> dict:\n",
    "    return {int(os.path.splitext(os.path.basename(f))[0]): os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.json')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(file_dict: dict):\n",
    "    song_dict = {}\n",
    "    for song_id, path in file_dict.items():\n",
    "        with open(path, 'r') as fp:\n",
    "            song_dict[song_id] = json.load(fp)\n",
    "    return song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "songid_to_file = get_song_files(os.path.join(dataset_path,'songs'))\n",
    "songid_to_song = load_songs(songid_to_file)"
   ]
  },
  {
   "source": [
    "## Cleanse lyrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def decontracted(phrase: str):\n",
    "    \"\"\"Remove English word contractions.\n",
    "\n",
    "    Gleaned from: https://stackoverflow.com/a/47091490\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"wanna\", \"want to\", phrase)\n",
    "    phrase = re.sub(r\"gotta\", \"got to\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def clean_lyric(lyric: str, preserve_lines: bool = False):\n",
    "    lyric = lyric.lower() # Convert to common case.\n",
    "    lyric = re.sub(r'\\[[^\\]]*\\]', '', lyric) # Remove paranthetical content \"[*]\", like markers for chorus and verses.\n",
    "    lyric = re.sub(r'\\([^\\)]*\\)', '', lyric) # Remove paranthetical content \"(*)\", like markers for chorus and verses.\n",
    "    lyric = lyric.strip() # Remove any extra newlines at the ends.\n",
    "    if preserve_lines:\n",
    "        lyric = re.sub(r\"(?:\\s*\\n\\s*)+\", r'\\n', lyric)\n",
    "        lyric = re.sub('\\n', ' NEWLINE ', lyric)\n",
    "    lyric = decontracted(lyric) # Remove contractions before tokenizer to handle special cases.\n",
    "    tokens = word_tokenize(lyric) # Split into word tokens.\n",
    "    tokens = [word for word in tokens if word.isalpha()] # Careful to remove punct after contractions.\n",
    "    if preserve_lines:\n",
    "        tokens = ['\\n' if 'NEWLINE' in word else word for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "songid_to_lyrics = {songid: clean_lyric(song['lyrics'], preserve_lines=True) for songid,song in songid_to_song.items()}"
   ]
  },
  {
   "source": [
    "## Build vocabulary list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique set of words.\n",
    "corpus = sorted(set(sum([lyrics for songid,lyrics in songid_to_lyrics.items()], [])))"
   ]
  },
  {
   "source": [
    "## Create integer mapping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mappings for: int <--> token\n",
    "int_to_token = {i: token for i,token in enumerate(corpus)}\n",
    "token_to_int = {token: i for i,token in int_to_token.items()}"
   ]
  },
  {
   "source": [
    "## "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aah',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abatido',\n",
       " 'abba',\n",
       " 'abide',\n",
       " 'abideth']"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "len(corpus)\n",
    "corpus[:10]"
   ]
  },
  {
   "source": [
    "## Embed lyrics as integers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "songid_to_embed = {songid: [token_to_int[token] for token in lyrics] for songid,lyrics in songid_to_lyrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['you', 'call', 'me', 'out', 'upon', 'the', 'waters', '\\n', 'the', 'great']\n[6232, 714, 3231, 3645, 5768, 5368, 5951, 0, 5368, 2320]\n"
     ]
    }
   ],
   "source": [
    "print(songid_to_lyrics[147168][:10])\n",
    "print(songid_to_embed[147168][:10])"
   ]
  },
  {
   "source": [
    "## Write embedding to pickle file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "store = {\n",
    "    'mapping': int_to_token,\n",
    "    'embedding': songid_to_embed,\n",
    "}\n",
    "store_file = os.path.join(dataset_path,'embedding.pickle')\n",
    "with open(store_file, 'wb') as fp:\n",
    "    pickle.dump(store, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "# Model Building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils.data"
   ]
  },
  {
   "source": [
    "## Construct Dataset class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorshipLyricDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Worhip Song dataset from Genius.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_file: str):\n",
    "        self.embedding_file = embedding_file\n",
    "\n",
    "        # Load the embedding.\n",
    "        with open(embedding_file, 'rb') as fp:\n",
    "            store = pickle.load(fp)\n",
    "        \n",
    "        self.corpus = store['mapping']\n",
    "        self.songid_to_embed = store['embedding']\n",
    "        self.idx_to_songid = {idx: songid for idx,songid in enumerate(sorted(self.songid_to_embed.keys()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.songid_to_embed)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        songid = self.idx_to_songid[idx]\n",
    "        lyric = {\n",
    "            'embed': torch.tensor(self.songid_to_embed[songid], dtype=torch.int),\n",
    "            'songid': songid,\n",
    "        }\n",
    "        return lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_dataset = WorshipLyricDataset(embedding_file=store_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'embed': tensor([5399, 2773, 2067, 5366, 3066,  863, 6101, 3499, 1898,    0, 2067, 5366,\n",
       "         3178, 5366, 1503, 3520, 2443,    1, 3849, 5465, 5025,    0, 2067, 5366,\n",
       "         3066,  599, 3072, 6101,  112,    0,  732, 2646, 5316, 6232,    1, 5081,\n",
       "         5316, 6232,    1, 5081,    0, 6232,  732, 2966, 3595, 3231,    0, 5376,\n",
       "         2773,    1, 3178,    0, 5008, 3595, 5368, 1098,    0, 2448, 2427, 3499,\n",
       "         2565,    0, 2448, 2427, 3499, 2060,    0,  179, 2543,  551, 4823,  236,\n",
       "         2273,    0,  732, 3520, 6232, 2461, 2538, 1204, 3645,    0,  179, 5376,\n",
       "         2773,    1, 2220,    0, 4601, 2067,    1, 1898,  179,    1, 2138,    0,\n",
       "         3942, 5366, 5368, 5077, 4893, 6065, 1681,    0,  697, 2733, 3564, 5905,\n",
       "          339,    0, 3611, 5765, 6235, 2466,  179, 4548,    0, 2646,  151, 2509,\n",
       "            0, 6232, 1500, 3520, 2443, 5465, 6155,    0, 2646,  732, 4613, 6235,\n",
       "         5311,    0, 2646, 6065,  395, 5376, 2687,    1, 2633, 6017, 6232,  714,\n",
       "            0, 2141,  236, 5376, 5465,  783, 6232, 6017, 6232, 1862,    0, 2509,\n",
       "         2773, 3422, 4734, 6232,  732, 2966, 3595, 3231,    0, 3575, 5376, 2773,\n",
       "            1,  863,    0, 6032, 2773, 4753,  179,  433, 5465,  395, 2125,    0,\n",
       "          697, 5376, 2773, 3499, 1217, 2067, 2543, 1473,    0, 2448, 3103, 5765,\n",
       "         5465, 2543, 3380,  179,    0,  266, 4694, 2556, 2543, 2398,    0, 3942,\n",
       "         5366, 4893,    0, 5368, 5177, 6065, 4711,   98,    0,  179, 5368, 3706,\n",
       "            0,  179, 5368, 3706, 6065, 1681,    0, 3706, 6065, 1681,    0, 5316,\n",
       "         3231, 2610,  732, 2646, 2610,  732, 2646, 3123, 2808,    0, 6017, 2646,\n",
       "         2443, 3480, 4625, 2543, 1828,    0, 6206, 2646, 4613, 6232, 1608,    0,\n",
       "          179, 2646, 5612,  179, 5902,  339,    0, 4870, 2553, 3422, 2398,    0,\n",
       "         2992, 3231, 5266, 6232, 5465,    1, 2138, 3564, 3313,    0, 2448, 2773,\n",
       "         5896, 2844, 5465, 1625, 6235, 5578, 3309,    0, 6206, 6206, 2448, 3128,\n",
       "         6232, 3371, 5357, 6232, 6065, 1760, 2897,    0, 2786, 3564, 5905,  339,\n",
       "            0, 3611, 5765,    0, 3611, 5765, 6235, 2466,  179, 4548,    0,  971,\n",
       "         3595], dtype=torch.int32),\n",
       " 'songid': 107810}"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "lyric_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}