{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python","version":"3.9.2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3bJ6ZZzry8Er"},"source":["# Model Building\n","\n","sources:\n","\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://towardsdatascience.com/generating-haiku-with-deep-learning-dbf5d18b4246\n","- https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"]},{"cell_type":"code","metadata":{"id":"bQRi6xm5y8Es","executionInfo":{"status":"ok","timestamp":1618250757944,"user_tz":240,"elapsed":274,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import os\n","import sys\n","import pickle\n","import torch\n","import torch.nn\n","import torch.optim\n","import torch.utils.data"],"execution_count":144,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"butZqcQOzOHF","executionInfo":{"status":"ok","timestamp":1618250758141,"user_tz":240,"elapsed":457,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"049eaf22-2fbd-4c07-a788-6ef45fe6c1df"},"source":["\"\"\"Google Drive\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","root_path = '/content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/'\n","dataset_path = './dataset'\n","store_file = os.path.join(root_path,dataset_path,'lyrics.pickle')"],"execution_count":145,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"kTBOCijsy8Es","executionInfo":{"status":"ok","timestamp":1618250758142,"user_tz":240,"elapsed":438,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"d9fbf0ca-6793-4ccf-e014-073a5896b1e3"},"source":["\"\"\"Offline Usage\"\"\"\n","# dataset_path = '../../dataset'\n","# store_file = os.path.join(dataset_path,'lyrics.pickle')"],"execution_count":146,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Offline Usage'"]},"metadata":{"tags":[]},"execution_count":146}]},{"cell_type":"markdown","metadata":{"id":"Cb-sVEkyy8Et"},"source":["## Construct Dataset class"]},{"cell_type":"code","metadata":{"id":"BcdpCHU5y8Et","executionInfo":{"status":"ok","timestamp":1618250758142,"user_tz":240,"elapsed":426,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["class WorshipLyricDataset(torch.utils.data.Dataset):\n","    \"\"\"Worhip Song dataset from Genius.\n","    \"\"\"\n","\n","    def __init__(self, path: str):\n","\n","        # Load the pre-processed pickle file.\n","        with open(path, 'rb') as fp:\n","            store = pickle.load(fp)\n","        \n","        # Unpack the pickle.\n","        self.index2token = store['index2token']\n","        self.token2index = store['token2index']\n","        self.counts = store['counts']\n","        self.corpus = store['corpus']\n","        \n","        # n_words = len(self.index2token)\n","        # self.vectors = [torch.nn.functional.one_hot(torch.LongTensor(vec), num_classes=n_words) for vec in store['vectors']]\n","        self.vectors = [torch.LongTensor(vec) for vec in store['vectors']]\n","        self.syllables = torch.nn.functional.one_hot(torch.LongTensor(store['syllables'])) # One-hot encoded syllabl counts.\n","\n","    def __len__(self):\n","        return len(self.vectors)\n","\n","    def __getitem__(self, idx):\n","        # lyric = {\n","        #     'vector': self.vectors[idx],\n","        #     'syllables': self.syllables[idx],\n","        # }\n","        return (self.vectors[idx], self.syllables[idx],)"],"execution_count":147,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"2qOn4O89y8Eu","executionInfo":{"status":"ok","timestamp":1618250758461,"user_tz":240,"elapsed":743,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Construct the data object.\n","dataset = WorshipLyricDataset(path=store_file)"],"execution_count":148,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkJxCjVK_aTw","executionInfo":{"status":"ok","timestamp":1618250758462,"user_tz":240,"elapsed":742,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def pad_collate(batch):\n","    \"\"\"Pad batches from dataloader.\n","\n","    This allows for more efficient padding,\n","    by only padding within each batch.\n","    \"\"\"\n","    sentences, syllables = zip(*batch)\n","    sen_lens = torch.LongTensor([len(vec) for vec in sentences])\n","    sen_pad = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n","    syllables = torch.stack(syllables) # Convert tuple of tensors to single 2D tensor.\n","    syllables = syllables.reshape(syllables.size(0),1,syllables.size(1)) # Convert to 3D.\n","    syllables = syllables.repeat_interleave(sen_pad.size(1), dim=1) # Duplicate syllable count for every word in each sentence.\n","    # print('pad_collate','sen_lens',sen_lens.size())\n","    # print('pad_collate','sen_pad',sen_pad.size())\n","    # print('pad_collate','syllables',syllables.size())\n","    return (sen_pad,syllables,sen_lens,)"],"execution_count":149,"outputs":[]},{"cell_type":"code","metadata":{"id":"IFMjBBc4y8Ev","executionInfo":{"status":"ok","timestamp":1618250758462,"user_tz":240,"elapsed":740,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Construct data loader.\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2, collate_fn=pad_collate)"],"execution_count":150,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XPFty72y8Ev"},"source":["## Construct Model\n","\n","We use an encode/decode architecture, with **encoder** and **decoder** layers, and also add an **attention** layer.\n","\n","This architecture was adapted from the wondeful PyTorch tutorial [\"NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\"](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n","\n","sources:\n","- https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html"]},{"cell_type":"markdown","metadata":{"id":"2XEogOJk_aTw"},"source":["### Encoder\n","The encoder outputs a value for each word in a given input sentence. For each input word, the encoder outputs the value vector and a hidden state, the hidden state is used for the next input word."]},{"cell_type":"code","metadata":{"id":"UQS_Mz8N_aTx","executionInfo":{"status":"ok","timestamp":1618250758463,"user_tz":240,"elapsed":739,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["class SentenceRNN(torch.nn.Module):\n","    def __init__(self, n_hidden: int, n_vocab: int, n_layers: int, dropout: float = 0., bidirectional: bool = False):\n","        super().__init__()\n","\n","        self.n_hidden = n_hidden\n","        self.n_vocab = n_vocab\n","        self.n_layers = n_layers\n","        self.bidirectional = bidirectional\n","        self.n_dir = 2 if bidirectional else 1\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=n_hidden,\n","        )\n","\n","        # LSTM layer.\n","        self.lstm = torch.nn.LSTM(\n","            input_size=n_hidden,\n","            hidden_size=n_hidden,\n","            num_layers=n_layers,\n","            dropout=dropout,\n","            bidirectional=bidirectional,\n","            batch_first=True,\n","            )\n","\n","        # Word mapping fully-connected layer.\n","        self.fc = torch.nn.Linear(in_features=n_hidden, out_features=n_vocab)\n","    \n","    def forward(self, sentences: torch.Tensor, lens: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor):\n","        \"\"\"\n","        Args:\n","            features (torch.Tensor): Embedded syllable features.\n","            sentences (torch.Tensor): Sentence word vectors.\n","            lens (torch.Tensor): True lengths of padded sentence vectors.\n","            hidden (torch.Tensor): Hidden state vector.\n","            cell (torch.Tensor): Cell state vector.\n","        \"\"\"\n","        \n","        # Embed the sentence vectors as floating-point.\n","        #\n","        # inputs: (batch_size, sentence_length,)\n","        sentences_embed = self.embed(sentences)\n","        # embedded: (batch_size, sentence_length, embed_dim,)\n","        # print('sentences_embed',sentences_embed.size())\n","\n","        # # Reshape the embedding so that it will fit into the LSTM.\n","        # sentences_embed = sentences_embed.view(sentences_embed.size(0),sentences_embed.size(1), -1)\n","        # print('sentences_embed',sentences_embed.size())\n","\n","        # Pack the embedding so that the paddings are ignored.\n","        sentences_embed_packed = torch.nn.utils.rnn.pack_padded_sequence(\n","            input=sentences_embed,\n","            lengths=lens, \n","            batch_first=True,\n","            enforce_sorted=False,\n","            )\n","        # print('sentences_embed_packed','data',sentences_embed_packed.data.size())\n","\n","        # Pass the input feature vector as the first step.\n","        output_packed, (hidden, cell) = self.lstm(sentences_embed_packed, (hidden,cell,))\n","        # print('output_packed',output_packed.data.size())\n","\n","        # Get padded output\n","        output_padded, output_lens = torch.nn.utils.rnn.pad_packed_sequence(output_packed, batch_first=True)\n","        # print('output_padded',output_padded.size())\n","\n","        # Obtain word-level classification.\n","        output_padded_fc = self.fc(output_padded)\n","        # print('output_padded_fc',output_padded_fc.size())\n","\n","        # Run packing on output layer.\n","        # return output_padded, output_lens, hidden\n","        return output_padded_fc, output_lens, (hidden, cell,)\n","\n","    def init_hc(self, batch_size: int, device: str = 'cpu'):\n","        return torch.zeros((self.n_layers*self.n_dir, batch_size, self.n_hidden), device=device)"],"execution_count":151,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_e9ODqNiy8Ew"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"7Fw74eIsy8Ex","executionInfo":{"status":"ok","timestamp":1618250758463,"user_tz":240,"elapsed":737,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import time\n","from contextlib import contextmanager\n","\n","class timecontext:\n","    \"\"\"Elapsed time context manager.\"\"\"\n","    def __enter__(self):\n","        self.seconds = time.time()\n","        return self\n","    \n","    def __exit__(self, type, value, traceback):\n","        self.seconds = time.time() - self.seconds\n","\n","@contextmanager\n","def timecontextprint(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    with timecontext() as t:\n","        yield t\n","    print(f\"{description}: {t.seconds} seconds\")"],"execution_count":152,"outputs":[]},{"cell_type":"code","metadata":{"id":"ib7AXBk3_aTy","executionInfo":{"status":"ok","timestamp":1618250758587,"user_tz":240,"elapsed":858,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import random\n","def train(decoder, loader, epochs, optimizer_decoder, criterion, device='cpu', teacher_force_ratio=0.5):\n","    decoder.to(device)\n","\n","    decoder.train()\n","\n","    for e in range(epochs):\n","        running_loss = 0.0\n","        for b,batch in enumerate(loader):\n","            sentences,syllables,sen_lens = batch\n","            sentences = sentences.to(device)\n","            syllables = syllables.to(device)\n","            sen_lens = sen_lens.to(device)\n","\n","            # Initialize hidden output.\n","            decoder_hidden = decoder.init_hc(32, device=device)\n","            decoder_cell = decoder.init_hc(32, device=device)\n","\n","            # Decode.\n","            # outputs, out_lens, _ = decoder(features, sentences, sen_lens, decoder_hidden, decoder_cell)\n","            SOS_token = dataset.token2index['<sos>']\n","            # decoder_input = SOS_token*torch.ones((sentences.size(0), sentences.size(1), 1,), dtype=torch.long, device=device)\n","            decoder_input = SOS_token*torch.ones((sentences.size(0), 1,), dtype=torch.long, device=device)\n","            # decoder_input_lens = torch.ones((sentences.size(0),), dtype=torch.long, device=device)\n","            decoder_input_lens = torch.ones((sentences.size(0),), dtype=torch.long)\n","\n","            # print('decoder_input',decoder_input.size())\n","            # print('decoder_input_lens',decoder_input_lens.size())\n","\n","            \n","            loss = 0\n","\n","            use_teacher_forcing = True if random.random() < teacher_force_ratio else False\n","\n","            # Teacher forcing.\n","            # Feed the target as the next input.\n","            if use_teacher_forcing:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # print(f\"[target {target_idx}] {targets.squeeze(dim=1)}\")\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # For teacher forcing set the input of the\n","                    # next round to be the current target.\n","                    decoder_input = targets.detach()\n","\n","            # No teacher forcing.\n","            # Feed the RNN predictions as the next input.\n","            else:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Get best prediction.\n","                    topv, topi = outputs.topk(1)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # Set the input of the next round to be the current prediction.\n","                    decoder_input = topi.squeeze(dim=-1).detach()\n","\n","            # Back-propagate, and step the optimizers.\n","            # if target_idx == 1: retain_graph = True\n","            # else: retain_graph = False\n","            loss.backward()\n","            optimizer_decoder.step()\n","            \n","            # Zero the gradients\n","            optimizer_decoder.zero_grad()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","            # print(f\"[batch {b}] loss: {running_loss}\")\n","\n","            # topv, topi = outputs.topk(1)\n","            # for i in range(5):\n","            #     s = ' '.join([dataset.index2token[idx] for idx in topi[i]])\n","            #     print(topi[i].view(-1))\n","            #     print(s)\n","            #     print()\n","\n","        # Report epoch results.\n","        print(f'Epoch {e}: loss {running_loss}')"],"execution_count":153,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2fPRgo--_aTy","executionInfo":{"status":"ok","timestamp":1618250758587,"user_tz":240,"elapsed":848,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"3b6f9589-76e8-4589-b352-45610db3ea31"},"source":["# Length of vocabulary.\n","n_words = len(dataset.index2token)\n","\n","# Decoder.\n","decoder = SentenceRNN(\n","    n_vocab=n_words,\n","    n_hidden=128,\n","    n_layers=1,\n","    dropout=0.1,\n","    bidirectional=False,\n",")"],"execution_count":154,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcoqyU4X_aTy","executionInfo":{"status":"ok","timestamp":1618250758588,"user_tz":240,"elapsed":837,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"9251f323-4437-4e55-f16a-8b411670e548"},"source":["# Set runtime device.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device: {device}\")"],"execution_count":155,"outputs":[{"output_type":"stream","text":["Device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qFD8N5y0QAoy","executionInfo":{"status":"ok","timestamp":1618250758588,"user_tz":240,"elapsed":825,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define path to decoder model storage.\n","decoder_store = os.path.join(root_path, dataset_path, 'decoder.pt')"],"execution_count":156,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"lCfDO1Ko_aTz","executionInfo":{"status":"ok","timestamp":1618250981643,"user_tz":240,"elapsed":223877,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"b62f6306-0aca-48f2-e6cd-560f0fd5eb6f"},"source":["load_from_file = True\n","\n","# Load model from store file.\n","if load_from_file and os.path.exists(decoder_store):\n","    decoder.load_state_dict(torch.load(decoder_store))\n","\n","else:\n","    # Learning parameters.\n","    epochs = 10\n","    lr = 1e-3\n","    print(f'Training decoder model: epochs={epochs}, lr={lr}, batches={len(dataloader)}')\n","\n","    # Train the model.\n","    # Display training time too.\n","    with timecontextprint():\n","        optim_decoder = torch.optim.Adam(decoder.parameters(), lr=lr)\n","        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","        train(decoder,\n","            loader=dataloader,\n","            epochs=epochs,\n","            optimizer_decoder=optim_decoder,\n","            criterion=criterion,\n","            device=device,\n","        )\n","\n","    # Store model state to file.\n","    decoder_store = os.path.join(root_path,dataset_path,'decoder.pt')\n","    torch.save(decoder.state_dict(), decoder_store)\n","    print(f'Saved decoder model: {decoder_store}')"],"execution_count":157,"outputs":[{"output_type":"stream","text":["Training decoder model: epochs=10, lr=0.001, batches=898\n","Epoch 0: loss 36063.487173080444\n","Epoch 1: loss 32784.56394195557\n","Epoch 2: loss 31954.298318862915\n","Epoch 3: loss 31770.73614692688\n","Epoch 4: loss 31322.788995742798\n","Epoch 5: loss 31091.779554367065\n","Epoch 6: loss 30705.09156036377\n","Epoch 7: loss 30519.26244354248\n","Epoch 8: loss 30194.965978622437\n","Epoch 9: loss 30001.43767929077\n","Elapsed time: 222.98313546180725 seconds\n","Saved decoder model: /content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/./dataset/decoder.pt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rNCyPzcSy8Ez"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"id":"QCzrY_eKDi7H","executionInfo":{"status":"ok","timestamp":1618250981644,"user_tz":240,"elapsed":223866,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import numpy as np\n","\n","def evaluate(decoder, seed='<sos>', max_length=None, device='cpu'):\n","    decoder.to(device)\n","    decoder.eval()\n","\n","    with torch.no_grad():\n","\n","        # Initialize hidden output.\n","        decoder_hidden = decoder.init_hc(1, device=device)\n","        decoder_cell = decoder.init_hc(1, device=device)\n","\n","        # Decode.\n","        EOS_token = dataset.token2index['<eos>']\n","        SOS_token = dataset.token2index['<sos>']\n","        seed_token = dataset.token2index.get(seed, SOS_token)\n","        decoder_input = seed_token*torch.ones((1, 1,), dtype=torch.long, device=device)\n","        decoder_input_lens = torch.ones((1,), dtype=torch.long)\n","\n","\n","        decoded_tokens = [seed]\n","        i = 0\n","        while True:\n","            outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","            probs = np.array(torch.nn.functional.softmax(outputs, dim=2).squeeze().cpu())\n","            probs = probs / probs.sum()\n","            topi = torch.tensor(np.random.choice(decoder.n_vocab, 1, p=probs, replace=False), dtype=torch.long, device=device).view(1,1,1)\n","\n","            if topi.item() == EOS_token:\n","                decoded_tokens.append('<eos>')\n","                break\n","            else:\n","                decoded_tokens.append(dataset.index2token[topi.item()])\n","\n","            # Stop if maximum sentence length has been reached.\n","            if max_length and len(decoded_tokens) == max_length:\n","                break\n","\n","            # Set the input of the next round to be the current prediction.\n","            decoder_input = topi.squeeze(dim=-1).detach()\n","            i += 1\n","\n","        return decoded_tokens"],"execution_count":158,"outputs":[]},{"cell_type":"code","metadata":{"id":"rAi8p8CP_aT0","executionInfo":{"status":"ok","timestamp":1618251038817,"user_tz":240,"elapsed":257,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def generate_song(num_lines: int, max_length: int = None, seeds: list = None) -> str:\n","    lines = []\n","    seed = '<sos>'\n","    if not seeds:\n","        seeds = [seed]*num_lines\n","    for i in range(num_lines):\n","        tokens = evaluate(decoder, max_length=max_length, seed=seeds[i], device=device)\n","        if tokens[0] != '<sos>': tokens = ['<sos>'] + tokens\n","        lines.append(' '.join(tokens))\n","    return '\\n'.join(lines)"],"execution_count":167,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omsy3YOj_aT0","executionInfo":{"status":"ok","timestamp":1618251053785,"user_tz":240,"elapsed":227,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"a42800c2-2e86-4cbf-9f6f-145e4621728c"},"source":["print(generate_song(num_lines=3, max_length=10))"],"execution_count":169,"outputs":[{"output_type":"stream","text":["<sos> have joy and rest with me tell this higher\n","<sos> am not running back to rise <eos>\n","<sos> lord inside me home <eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXuEKFKz_aT1","executionInfo":{"status":"ok","timestamp":1618251054799,"user_tz":240,"elapsed":312,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"a55f3808-d8df-47b0-8ac5-560b6b9b09d8"},"source":["print(generate_song(num_lines=3, seeds=['god', 'we', 'they']))"],"execution_count":170,"outputs":[{"output_type":"stream","text":["<sos> god want that love is work yeah yeah <eos>\n","<sos> we write hoping caught warning yet as not give me go <eos>\n","<sos> they blown human searching defeated preaching blew is the clear <eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65_gGRFgNZKG","executionInfo":{"status":"ok","timestamp":1618251055562,"user_tz":240,"elapsed":215,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"b6be5963-e6ca-4940-e8a2-5d7d1690d1b9"},"source":["print(generate_song(num_lines=3, seeds=['we', 'they', 'you']))"],"execution_count":171,"outputs":[{"output_type":"stream","text":["<sos> we cry prone out of itself <eos>\n","<sos> they would really remember just blind <eos>\n","<sos> you planned sung let me go to you <eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wbO3PT46Ne8O","executionInfo":{"status":"ok","timestamp":1618250981849,"user_tz":240,"elapsed":224060,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":[""],"execution_count":162,"outputs":[]}]}