{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python","version":"3.9.2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3bJ6ZZzry8Er"},"source":["# Model Building\n","\n","sources:\n","\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://towardsdatascience.com/generating-haiku-with-deep-learning-dbf5d18b4246\n","- https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"]},{"cell_type":"code","metadata":{"id":"bQRi6xm5y8Es","executionInfo":{"status":"ok","timestamp":1618259692559,"user_tz":240,"elapsed":3797,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import os\n","import sys\n","import pickle\n","import torch\n","import torch.nn\n","import torch.optim\n","import torch.utils.data"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"butZqcQOzOHF","executionInfo":{"status":"ok","timestamp":1618259711714,"user_tz":240,"elapsed":22937,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"66bb3ee4-d4e5-4b0f-e45e-14c1cf07d7f6"},"source":["\"\"\"Google Drive\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","root_path = '/content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/'\n","dataset_path = './dataset'\n","store_file = os.path.join(root_path,dataset_path,'lyrics.pickle')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"kTBOCijsy8Es","executionInfo":{"status":"ok","timestamp":1618259711715,"user_tz":240,"elapsed":22917,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"2d3303fb-e8aa-47fd-fc9f-e05ef5213678"},"source":["\"\"\"Offline Usage\"\"\"\n","# dataset_path = '../../dataset'\n","# store_file = os.path.join(dataset_path,'lyrics.pickle')"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Offline Usage'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Cb-sVEkyy8Et"},"source":["## Construct Dataset class\n","\n","We construct a PyTorch dataset class which aids in loading the pre-processed song lyrics.\n","\n","The pre-processed lyrics come with the following attributes:\n","\n","- `index2token`: Maps token integer ID to actual token\n","- `token2index`: Maps token to integer ID\n","- `counts`: Token frequencies\n","- `corpus`: List of tokenized lyrics for each sentence\n","- `vectors`: List of token ID tensors for each sentence"]},{"cell_type":"code","metadata":{"id":"BcdpCHU5y8Et","executionInfo":{"status":"ok","timestamp":1618259711716,"user_tz":240,"elapsed":22901,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["class WorshipLyricDataset(torch.utils.data.Dataset):\n","    \"\"\"Worhip Song dataset from Genius.\n","    \"\"\"\n","\n","    def __init__(self, path: str):\n","\n","        # Load the pre-processed pickle file.\n","        with open(path, 'rb') as fp:\n","            store = pickle.load(fp)\n","        \n","        # Unpack the pickle.\n","        self.index2token = store['index2token']\n","        self.token2index = store['token2index']\n","        self.counts = store['counts']\n","        self.corpus = store['corpus']\n","        self.vectors = [torch.LongTensor(vec) for vec in store['vectors']]\n","        self.syllables = torch.nn.functional.one_hot(torch.LongTensor(store['syllables'])) # One-hot encoded syllabl counts.\n","\n","    def __len__(self):\n","        return len(self.vectors)\n","\n","    def __getitem__(self, idx):\n","        return (self.vectors[idx], self.syllables[idx],)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"2qOn4O89y8Eu","executionInfo":{"status":"ok","timestamp":1618259715294,"user_tz":240,"elapsed":26476,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Construct the data object.\n","dataset = WorshipLyricDataset(path=store_file)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8VO8Xnd1z4_"},"source":["To support training with variable-length sentences we must pad the input sentences on a per-batch basis. To do this in conjunction with a data loader, we define a \"collate\" function which pads the sentences within each batch."]},{"cell_type":"code","metadata":{"id":"IkJxCjVK_aTw","executionInfo":{"status":"ok","timestamp":1618259715295,"user_tz":240,"elapsed":26474,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def pad_collate(batch):\n","    \"\"\"Pad batches from dataloader.\n","\n","    This allows for more efficient padding,\n","    by only padding within each batch.\n","    \"\"\"\n","    sentences, syllables = zip(*batch)\n","    sen_lens = torch.LongTensor([len(vec) for vec in sentences])\n","    sen_pad = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n","    syllables = torch.stack(syllables) # Convert tuple of tensors to single 2D tensor.\n","    syllables = syllables.reshape(syllables.size(0),1,syllables.size(1)) # Convert to 3D.\n","    syllables = syllables.repeat_interleave(sen_pad.size(1), dim=1) # Duplicate syllable count for every word in each sentence.\n","    return (sen_pad,syllables,sen_lens,)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dc1BcftZ2GQb"},"source":["With the dataset and collation function defined we can now construct a `dataloader`, which will allow us to iterate over the lyrics dataset in batches. All training will be done using this loader object."]},{"cell_type":"code","metadata":{"id":"IFMjBBc4y8Ev","executionInfo":{"status":"ok","timestamp":1618259715295,"user_tz":240,"elapsed":26472,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Construct data loader.\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2, collate_fn=pad_collate)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2XEogOJk_aTw"},"source":["## Decoder model\n","\n","We define a novel RNN decoder architecture, called `SentenceRNN`, to generate next-word prediction for religious music.\n","\n","This architecture was adapted from the wondeful PyTorch tutorial [\"NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\"](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n","\n","The anatomy of our model consts of:\n","\n","1. Embedding input layer\n","1. Unidirectional single-layer LSTM\n","1. Fully-connected output layer\n","\n","The model is generalized to support variable-length sequences by requiring inputs to be **zero-padded** and then it subsequently **packs** the padded input so that padding tokens are ignored by the internal LSTM layers.\n","\n","For each forward pass of the model it outputs the **current token prediction** (from the fully-connected layer), the **lengths of the sequence predictions** (required for packed padded input sequences), and the **LSTM output hidden and cell states**. This allows the LSTM hidden and cell states to be fed back into subsequent forward passes to retain sequence memory."]},{"cell_type":"code","metadata":{"id":"UQS_Mz8N_aTx","executionInfo":{"status":"ok","timestamp":1618259715295,"user_tz":240,"elapsed":26460,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["class SentenceRNN(torch.nn.Module):\n","    \n","    def __init__(self, n_hidden: int, n_vocab: int, n_layers: int, dropout: float = 0., bidirectional: bool = False):\n","        super().__init__()\n","\n","        self.n_hidden = n_hidden\n","        self.n_vocab = n_vocab\n","        self.n_layers = n_layers\n","        self.bidirectional = bidirectional\n","        self.n_dir = 2 if bidirectional else 1\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=n_hidden,\n","        )\n","\n","        # LSTM layer.\n","        self.lstm = torch.nn.LSTM(\n","            input_size=n_hidden,\n","            hidden_size=n_hidden,\n","            num_layers=n_layers,\n","            dropout=dropout,\n","            bidirectional=bidirectional,\n","            batch_first=True,\n","            )\n","\n","        # Word mapping fully-connected layer.\n","        self.fc = torch.nn.Linear(in_features=n_hidden, out_features=n_vocab)\n","    \n","\n","    def forward(self, sentences: torch.Tensor, lens: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor):\n","        \"\"\"\n","        Args:\n","            sentences (torch.Tensor): Sentence word vectors.\n","            lens (torch.Tensor): True lengths of padded sentence vectors.\n","            hidden (torch.Tensor): Hidden state vector.\n","            cell (torch.Tensor): Cell state vector.\n","        \"\"\"\n","        \n","        # Embed the sentence vectors as floating-point.\n","        #\n","        # inputs: (batch_size, sentence_length,)\n","        sentences_embed = self.embed(sentences)\n","        # embedded: (batch_size, sentence_length, embed_dim,)\n","\n","        # Pack the embedding so that the paddings are ignored.\n","        sentences_embed_packed = torch.nn.utils.rnn.pack_padded_sequence(\n","            input=sentences_embed,\n","            lengths=lens, \n","            batch_first=True,\n","            enforce_sorted=False,\n","            )\n","\n","        # Pass the input feature vector as the first step.\n","        output_packed, (hidden, cell) = self.lstm(sentences_embed_packed, (hidden,cell,))\n","\n","        # Get padded output\n","        output_padded, output_lens = torch.nn.utils.rnn.pad_packed_sequence(output_packed, batch_first=True)\n","\n","        # Obtain token-level classification.\n","        output_padded_fc = self.fc(output_padded)\n","\n","        # Run packing on output layer.\n","        return output_padded_fc, output_lens, (hidden, cell,)\n","\n","    def init_hc(self, batch_size: int, device: str = 'cpu') -> torch.Tensor:\n","        \"\"\"Helepr to zero-initialize hidden and cell state tensors.\"\"\"\n","        return torch.zeros((self.n_layers*self.n_dir, batch_size, self.n_hidden), device=device)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_e9ODqNiy8Ew"},"source":["## Train\n","\n","Here we define some helper classes and functions for timing the training rounds."]},{"cell_type":"code","metadata":{"id":"7Fw74eIsy8Ex","executionInfo":{"status":"ok","timestamp":1618259715296,"user_tz":240,"elapsed":26448,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import time\n","from contextlib import contextmanager\n","\n","class timecontext:\n","    \"\"\"Elapsed time context manager.\"\"\"\n","    def __enter__(self):\n","        self.seconds = time.time()\n","        return self\n","    \n","    def __exit__(self, type, value, traceback):\n","        self.seconds = time.time() - self.seconds\n","\n","@contextmanager\n","def timecontextprint(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    with timecontext() as t:\n","        yield t\n","    print(f\"{description}: {t.seconds} seconds\")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rN445XAnnXlg"},"source":["The training itself is done by initializing the LSTM hidden and cell states to zero. Then to generalize all sentence structures we always set the first token to be run through the model as the start-of-sentence (SOS) token. \n","\n","To better generalize the next-token predictions we apply a technique known as \"teacher forcing\". In teacher forcing we pass the known next-token target value at each step as the input to the next decoder step. This forces the decoder to learn using the proper next-token rather than solely based on the predictions at each step. To increase generalization performance further we randomly apply teacher forcing for each batch based on a probability distribution (by default 50% probability). "]},{"cell_type":"code","metadata":{"id":"ib7AXBk3_aTy","executionInfo":{"status":"ok","timestamp":1618259715478,"user_tz":240,"elapsed":26628,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import random\n","def train(decoder, loader, epochs, optimizer_decoder, criterion, device='cpu', teacher_force_ratio=0.5):\n","    decoder.to(device)\n","    decoder.train()\n","    for e in range(epochs):\n","        running_loss = 0.0\n","        for b,batch in enumerate(loader):\n","            sentences,syllables,sen_lens = batch\n","            sentences = sentences.to(device)\n","            syllables = syllables.to(device)\n","            sen_lens = sen_lens.to(device)\n","\n","            # Initialize hidden output.\n","            decoder_hidden = decoder.init_hc(32, device=device)\n","            decoder_cell = decoder.init_hc(32, device=device)\n","\n","            # Setup initial decoder inputs.\n","            SOS_token = dataset.token2index['<sos>']\n","            decoder_input = SOS_token*torch.ones((sentences.size(0), 1,), dtype=torch.long, device=device)\n","            decoder_input_lens = torch.ones((sentences.size(0),), dtype=torch.long)\n","\n","            # Initialize batch loss to zero.\n","            loss = 0\n","\n","            # Determine if teacher-forcing should be used for this batch.\n","            use_teacher_forcing = True if random.random() < teacher_force_ratio else False\n","\n","            # Teacher forcing.\n","            # Feed the target as the next input.\n","            if use_teacher_forcing:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # For teacher forcing set the input of the\n","                    # next round to be the current target.\n","                    decoder_input = targets.detach()\n","\n","            # No teacher forcing.\n","            # Feed the RNN predictions as the next input.\n","            else:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Get best prediction.\n","                    topv, topi = outputs.topk(1)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # Set the input of the next round to be the current prediction.\n","                    decoder_input = topi.squeeze(dim=-1).detach()\n","\n","            # Back-propagate, and step the optimizers.\n","            loss.backward()\n","            optimizer_decoder.step()\n","            \n","            # Zero the gradients\n","            optimizer_decoder.zero_grad()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","        # Report epoch results.\n","        print(f'[epoch {e}]: loss {running_loss}')"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fPRgo--_aTy","executionInfo":{"status":"ok","timestamp":1618259715478,"user_tz":240,"elapsed":26625,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Length of vocabulary.\n","n_words = len(dataset.index2token)\n","\n","# Decoder.\n","decoder = SentenceRNN(\n","    n_vocab=n_words,\n","    n_hidden=256,\n","    n_layers=1,\n",")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0wo-azdpYoj"},"source":["To speed-up training PyTorch allows us to leverage a GPU, using CUDA, if one is available. Since training a CNN can be computationally intensive we prefer to use a GPU for speed, but will revert to using the CPU if necessary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcoqyU4X_aTy","executionInfo":{"status":"ok","timestamp":1618259715614,"user_tz":240,"elapsed":26749,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"14194a8d-d9c6-41ee-fe46-2c90760afe73"},"source":["# Set runtime device.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device: {device}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Device: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qrwgQjIs2kp3"},"source":["With the model defined we can now train it on the lyrics dataset.\n","\n","We define a set of training hyperparameters `epochs` and `lr` which are number of training iterations and optimizer learning rate respectively.\n","\n","To speed-up subsequent runs, we also save the trained model to a file. This allows us to train the model once, and then simply load the pre-trained model (using the flag `load_from_file = True`) if the Jupyter notebook is run multiple times during a single session."]},{"cell_type":"code","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"lCfDO1Ko_aTz","executionInfo":{"status":"ok","timestamp":1618259942344,"user_tz":240,"elapsed":253466,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"7105fd1c-36c9-4f3a-9980-8a98e33cbd44"},"source":["# Define path to decoder model storage.\n","load_from_file = True\n","decoder_store = os.path.join(root_path, dataset_path, 'decoder.pt')\n","\n","# Load model from store file.\n","if load_from_file and os.path.exists(decoder_store):\n","    decoder.load_state_dict(torch.load(decoder_store))\n","\n","else:\n","    # Learning parameters.\n","    epochs = 10\n","    lr = 1e-3\n","    print(f'Training decoder model: epochs={epochs}, lr={lr}, batches={len(dataloader)}')\n","\n","    # Train the model.\n","    # Display training time too.\n","    with timecontextprint():\n","        optim_decoder = torch.optim.Adam(decoder.parameters(), lr=lr)\n","        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","        train(decoder,\n","            loader=dataloader,\n","            epochs=epochs,\n","            optimizer_decoder=optim_decoder,\n","            criterion=criterion,\n","            device=device,\n","        )\n","\n","    # Store model state to file.\n","    decoder_store = os.path.join(root_path,dataset_path,'decoder.pt')\n","    torch.save(decoder.state_dict(), decoder_store)\n","    print(f'Saved decoder model: {decoder_store}')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Training decoder model: epochs=10, lr=0.001, batches=898\n","[epoch 0]: loss 35091.922761917114\n","[epoch 1]: loss 32093.397901535034\n","[epoch 2]: loss 31068.27512550354\n","[epoch 3]: loss 30753.926628112793\n","[epoch 4]: loss 30132.63254737854\n","[epoch 5]: loss 29897.695526123047\n","[epoch 6]: loss 29800.166801452637\n","[epoch 7]: loss 29300.294973373413\n","[epoch 8]: loss 28935.599615097046\n","[epoch 9]: loss 28534.963705062866\n","Elapsed time: 226.15394949913025 seconds\n","Saved decoder model: /content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/./dataset/decoder.pt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rNCyPzcSy8Ez"},"source":["## Evaluate\n","\n","Now that the model has been trained we can use it to generate song lyrics.\n","\n","We define a helper function to evaluate the decoder model. The process of decoder evaluation is actually very similar to training. The difference is that for each token prediction we randomly choose the predicted token based on the normalized probability distribution of the prediction set. We also do not employ teacher forcing since the next-token at each step is unknown.\n","\n","The evaluation process continuously loops through next-token predictions until either an end-of-sentence (EOS) token or a maximum decoded token length is reached."]},{"cell_type":"code","metadata":{"id":"QCzrY_eKDi7H","executionInfo":{"status":"ok","timestamp":1618259942344,"user_tz":240,"elapsed":253450,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import numpy as np\n","from typing import List\n","\n","def evaluate(decoder, seed='<sos>', max_length=None, device='cpu') -> List[str]:\n","    \"\"\"Generate a sequence of tokens using the decoder and a starting seed.\"\"\"\n","    decoder.to(device)\n","    decoder.eval()\n","    with torch.no_grad():\n","\n","        # Initialize hidden output.\n","        decoder_hidden = decoder.init_hc(1, device=device)\n","        decoder_cell = decoder.init_hc(1, device=device)\n","\n","        # Setup initial decoder inputs.\n","        EOS_index = dataset.token2index['<eos>']\n","        SOS_index = dataset.token2index['<sos>']\n","        seed_token = dataset.token2index.get(seed, SOS_index)\n","        decoder_input = seed_token*torch.ones((1, 1,), dtype=torch.long, device=device)\n","        decoder_input_lens = torch.ones((1,), dtype=torch.long)\n","\n","        # Always initialize the deocded tokens with an SOS token.\n","        decoded_tokens = [seed]\n","\n","        # Loop indefinitely until token list is generated.\n","        while True:\n","\n","            # Run current inputs, hidden and cell states through the decoder.\n","            outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","            # Normalize the probability distribution of the current prediction.\n","            probs = np.array(torch.nn.functional.softmax(outputs, dim=2).squeeze().cpu())\n","            probs = probs / probs.sum()\n","\n","            # Choose the top prediction from the normalized probability disrtribution above.\n","            topi = torch.tensor(np.random.choice(decoder.n_vocab, 1, p=probs, replace=False), dtype=torch.long, device=device).view(1,1,1)\n","\n","            # Add the current token to the decoded list.\n","            decoded_tokens.append(dataset.index2token[topi.item()])\n","\n","            # Stop if current token is EOS or maximum sentence length has been reached.\n","            if (topi.item() == EOS_index) or (max_length and len(decoded_tokens) == max_length):\n","                break\n","\n","            # Set the input of the next round to be the current prediction.\n","            decoder_input = topi.squeeze(dim=-1).detach()\n","\n","        return decoded_tokens"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykEZeefsuvb2"},"source":["We also define a helper function to generate song lyrics, composed of multiple lines, by inputting a set of line-count, optional maximum sentence length, and optional seed list for each sentence. This generates a sequence of tokens for each line of the song and then subsequently joins all lines into a single newline-delimited string."]},{"cell_type":"code","metadata":{"id":"rAi8p8CP_aT0","executionInfo":{"status":"ok","timestamp":1618259942345,"user_tz":240,"elapsed":253447,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def generate_song(num_lines: int, max_length: int = None, seeds: list = None) -> str:\n","    \"\"\"Helper to generate song lyrics given constraints.\"\"\"\n","\n","    # Build list of line seeds if none were provided.\n","    seed = '<sos>'\n","    if not seeds:\n","        seeds = [seed]*num_lines\n","\n","    # Generate predictions for each line.\n","    lines = []\n","    for i in range(num_lines):\n","        tokens = evaluate(decoder, max_length=max_length, seed=seeds[i], device=device)\n","        if tokens[0] != '<sos>': tokens = ['<sos>'] + tokens\n","        lines.append(' '.join(tokens))\n","\n","    # Join lines together and return as single string.\n","    return '\\n'.join(lines)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omsy3YOj_aT0","executionInfo":{"status":"ok","timestamp":1618259942345,"user_tz":240,"elapsed":253443,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"ddd720d4-f56f-4323-ba71-82afdb809abb"},"source":["print(generate_song(num_lines=3, max_length=10))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<sos> are my help in my affection <eos>\n","<sos> me that is starting <eos>\n","<sos> don t still you why i do not deserve\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXuEKFKz_aT1","executionInfo":{"status":"ok","timestamp":1618259942346,"user_tz":240,"elapsed":253430,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"4de5e822-13ee-46bb-fb80-2f425add2872"},"source":["print(generate_song(num_lines=3, seeds=['god', 'we', 'they']))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["<sos> god wake deeper deep else <eos>\n","<sos> we exalt dwelling faithfulness watch generations win <eos>\n","<sos> they can take strangers home away <eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65_gGRFgNZKG","executionInfo":{"status":"ok","timestamp":1618260017586,"user_tz":240,"elapsed":259,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"42b4616e-2e23-45c3-f517-41d037cad993"},"source":["print(generate_song(num_lines=3, seeds=['we', 'give', 'you',]))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["<sos> we bear dancing <eos>\n","<sos> give create disease buried there hand to know your love sings me <eos>\n","<sos> you breathes rest and messiah <eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wbO3PT46Ne8O","executionInfo":{"status":"ok","timestamp":1618259942347,"user_tz":240,"elapsed":253406,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":[""],"execution_count":19,"outputs":[]}]}