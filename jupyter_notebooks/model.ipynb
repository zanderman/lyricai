{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"colab":{"name":"model.ipynb","provenance":[]},"language_info":{"name":"python","version":"3.9.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3bJ6ZZzry8Er"},"source":["# Model Building\n","\n","sources:\n","\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://towardsdatascience.com/generating-haiku-with-deep-learning-dbf5d18b4246\n","- https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"]},{"cell_type":"code","metadata":{"id":"bQRi6xm5y8Es","executionInfo":{"status":"ok","timestamp":1617892488018,"user_tz":240,"elapsed":395,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}}},"source":["import os\n","import sys\n","import pickle\n","import torch\n","import torch.nn\n","import torch.optim\n","import torch.utils.data"],"execution_count":645,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"butZqcQOzOHF","executionInfo":{"status":"ok","timestamp":1617892507139,"user_tz":240,"elapsed":1105,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"a3c3e803-d2f1-48c6-fde8-a99a53fb42a0"},"source":["\"\"\"Google Drive\"\"\"\n","# # Mount Juliet's google drive\n","# from google.colab import drive\n","# drive.mount('/content/gdrive/')\n","# sys_path = '/content/gdrive/My Drive/project_ece_5424/'\n","# sys.path.append(sys_path)\n","# dataset_path = 'dataset'\n","# store_file = os.path.join(sys_path, dataset_path, 'embedding.pickle')"],"execution_count":646,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Google Drive'"]},"metadata":{},"execution_count":646}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kTBOCijsy8Es","executionInfo":{"status":"ok","timestamp":1617892397557,"user_tz":240,"elapsed":1947,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"b825ceee-5e5d-4486-a255-62f2c33cff0f"},"source":["\"\"\"Offline Usage\"\"\"\n","dataset_path = '../../dataset'\n","store_file = os.path.join(dataset_path,'lyrics.pickle')"],"execution_count":647,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cb-sVEkyy8Et"},"source":["## Construct Dataset class"]},{"cell_type":"code","metadata":{"id":"BcdpCHU5y8Et"},"source":["class WorshipLyricDataset(torch.utils.data.Dataset):\n","    \"\"\"Worhip Song dataset from Genius.\n","    \"\"\"\n","\n","    def __init__(self, path: str):\n","\n","        # Load the pre-processed pickle file.\n","        with open(path, 'rb') as fp:\n","            store = pickle.load(fp)\n","        \n","        # Unpack the pickle.\n","        self.index2token = store['index2token']\n","        self.token2index = store['token2index']\n","        self.counts = store['counts']\n","        self.corpus = store['corpus']\n","        \n","        # n_words = len(self.index2token)\n","        # self.vectors = [torch.nn.functional.one_hot(torch.LongTensor(vec), num_classes=n_words) for vec in store['vectors']]\n","        self.vectors = [torch.LongTensor(vec) for vec in store['vectors']]\n","        self.syllables = torch.nn.functional.one_hot(torch.LongTensor(store['syllables'])) # One-hot encoded syllabl counts.\n","\n","    def __len__(self):\n","        return len(self.vectors)\n","\n","    def __getitem__(self, idx):\n","        # lyric = {\n","        #     'vector': self.vectors[idx],\n","        #     'syllables': self.syllables[idx],\n","        # }\n","        return (self.vectors[idx], self.syllables[idx],)"],"execution_count":648,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"2qOn4O89y8Eu","executionInfo":{"status":"error","timestamp":1617892518083,"user_tz":240,"elapsed":502,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"9e8ebb09-3dbe-42c8-859e-82e751dd51b8"},"source":["# Construct the data object.\n","dataset = WorshipLyricDataset(path=store_file)"],"execution_count":649,"outputs":[]},{"cell_type":"code","execution_count":650,"metadata":{},"outputs":[],"source":["def pad_collate(batch):\n","    \"\"\"Pad batches from dataloader.\n","\n","    This allows for more efficient padding,\n","    by only padding within each batch.\n","    \"\"\"\n","    sentences, syllables = zip(*batch)\n","    sen_lens = torch.LongTensor([len(vec) for vec in sentences])\n","    sen_pad = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n","    syllables = torch.stack(syllables) # Convert tuple of tensors to single 2D tensor.\n","    syllables = syllables.reshape(syllables.size(0),1,syllables.size(1)) # Convert to 3D.\n","    syllables = syllables.repeat_interleave(sen_pad.size(1), dim=1) # Duplicate syllable count for every word in each sentence.\n","    # print('pad_collate','sen_lens',sen_lens.size())\n","    # print('pad_collate','sen_pad',sen_pad.size())\n","    # print('pad_collate','syllables',syllables.size())\n","    return (sen_pad,syllables,sen_lens,)"]},{"cell_type":"code","metadata":{"id":"IFMjBBc4y8Ev"},"source":["# Construct data loader.\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=pad_collate)"],"execution_count":651,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XPFty72y8Ev"},"source":["## Construct Model\n","\n","We use an encode/decode architecture, with **encoder** and **decoder** layers, and also add an **attention** layer.\n","\n","This architecture was adapted from the wondeful PyTorch tutorial [\"NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\"](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n","\n","sources:\n","- https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html"]},{"source":["### Encoder\n","The encoder outputs a value for each word in a given input sentence. For each input word, the encoder outputs the value vector and a hidden state, the hidden state is used for the next input word."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":652,"metadata":{},"outputs":[],"source":["class SentenceRNN(torch.nn.Module):\n","    def __init__(self, n_hidden: int, n_vocab: int, n_layers: int, dropout: float = 0., bidirectional: bool = False):\n","        super().__init__()\n","\n","        self.n_hidden = n_hidden\n","        self.n_vocab = n_vocab\n","        self.n_layers = n_layers\n","        self.bidirectional = bidirectional\n","        self.n_dir = 2 if bidirectional else 1\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=n_hidden,\n","        )\n","\n","        # LSTM layer.\n","        self.lstm = torch.nn.LSTM(\n","            input_size=n_hidden,\n","            hidden_size=n_hidden,\n","            num_layers=n_layers,\n","            dropout=dropout,\n","            bidirectional=bidirectional,\n","            batch_first=True,\n","            )\n","\n","        # Word mapping fully-connected layer.\n","        self.fc = torch.nn.Linear(in_features=n_hidden, out_features=n_vocab)\n","    \n","    def forward(self, sentences: torch.Tensor, lens: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor):\n","        \"\"\"\n","        Args:\n","            features (torch.Tensor): Embedded syllable features.\n","            sentences (torch.Tensor): Sentence word vectors.\n","            lens (torch.Tensor): True lengths of padded sentence vectors.\n","            hidden (torch.Tensor): Hidden state vector.\n","            cell (torch.Tensor): Cell state vector.\n","        \"\"\"\n","        \n","        # Embed the sentence vectors as floating-point.\n","        #\n","        # inputs: (batch_size, sentence_length,)\n","        sentences_embed = self.embed(sentences)\n","        # embedded: (batch_size, sentence_length, embed_dim,)\n","        # print('sentences_embed',sentences_embed.size())\n","\n","        # # Reshape the embedding so that it will fit into the LSTM.\n","        # sentences_embed = sentences_embed.view(sentences_embed.size(0),sentences_embed.size(1), -1)\n","        # print('sentences_embed',sentences_embed.size())\n","\n","        # Pack the embedding so that the paddings are ignored.\n","        sentences_embed_packed = torch.nn.utils.rnn.pack_padded_sequence(\n","            input=sentences_embed,\n","            lengths=lens, \n","            batch_first=True,\n","            enforce_sorted=False,\n","            )\n","        # print('sentences_embed_packed','data',sentences_embed_packed.data.size())\n","\n","        # Pass the input feature vector as the first step.\n","        output_packed, (hidden, cell) = self.lstm(sentences_embed_packed, (hidden,cell,))\n","        # print('output_packed',output_packed.data.size())\n","\n","        # Get padded output\n","        output_padded, output_lens = torch.nn.utils.rnn.pad_packed_sequence(output_packed, batch_first=True)\n","        # print('output_padded',output_padded.size())\n","\n","        # Obtain word-level classification.\n","        output_padded_fc = self.fc(output_padded)\n","        # print('output_padded_fc',output_padded_fc.size())\n","\n","        # Run packing on output layer.\n","        # return output_padded, output_lens, hidden\n","        return output_padded_fc, output_lens, (hidden, cell,)\n","\n","    def init_hc(self, batch_size: int, device: str = 'cpu'):\n","        return torch.zeros((self.n_layers*self.n_dir, batch_size, self.n_hidden), device=device)"]},{"cell_type":"markdown","metadata":{"id":"_e9ODqNiy8Ew"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"7Fw74eIsy8Ex"},"source":["import time\n","from contextlib import contextmanager\n","\n","class timecontext:\n","    \"\"\"Elapsed time context manager.\"\"\"\n","    def __enter__(self):\n","        self.seconds = time.time()\n","        return self\n","    \n","    def __exit__(self, type, value, traceback):\n","        self.seconds = time.time() - self.seconds\n","\n","@contextmanager\n","def timecontextprint(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    with timecontext() as t:\n","        yield t\n","    print(f\"{description}: {t.seconds} seconds\")"],"execution_count":653,"outputs":[]},{"cell_type":"code","execution_count":654,"metadata":{},"outputs":[],"source":["import random\n","def train(decoder, loader, epochs, optimizer_decoder, criterion, device='cpu', teacher_force_ratio=0.5):\n","    decoder.to(device)\n","\n","    decoder.train()\n","\n","    for e in range(epochs):\n","        running_loss = 0.0\n","        for b,batch in enumerate(loader):\n","            sentences,syllables,sen_lens = batch\n","\n","            # Initialize hidden output.\n","            decoder_hidden = decoder.init_hc(32, device=device)\n","            decoder_cell = decoder.init_hc(32, device=device)\n","\n","            # Decode.\n","            # outputs, out_lens, _ = decoder(features, sentences, sen_lens, decoder_hidden, decoder_cell)\n","            SOS_token = dataset.token2index['<sos>']\n","            # decoder_input = SOS_token*torch.ones((sentences.size(0), sentences.size(1), 1,), dtype=torch.long, device=device)\n","            decoder_input = SOS_token*torch.ones((sentences.size(0), 1,), dtype=torch.long, device=device)\n","            decoder_input_lens = torch.ones((sentences.size(0),), dtype=torch.long, device=device)\n","\n","            # print('decoder_input',decoder_input.size())\n","            # print('decoder_input_lens',decoder_input_lens.size())\n","\n","            \n","            loss = 0\n","\n","            use_teacher_forcing = True if random.random() < teacher_force_ratio else False\n","\n","            # Teacher forcing.\n","            # Feed the target as the next input.\n","            if use_teacher_forcing:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # print(f\"[target {target_idx}] {targets.squeeze(dim=1)}\")\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # For teacher forcing set the input of the\n","                    # next round to be the current target.\n","                    decoder_input = targets.detach()\n","\n","            # No teacher forcing.\n","            # Feed the RNN predictions as the next input.\n","            else:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Get best prediction.\n","                    topv, topi = outputs.topk(1)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # For teacher forcing set the input of the\n","                    # next round to be the current target.\n","                    decoder_input = topi.squeeze(dim=-1).detach()\n","\n","            # Back-propagate, and step the optimizers.\n","            # if target_idx == 1: retain_graph = True\n","            # else: retain_graph = False\n","            loss.backward()\n","            optimizer_decoder.step()\n","            \n","            # Zero the gradients\n","            optimizer_decoder.zero_grad()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","            # print(f\"[batch {b}] loss: {running_loss}\")\n","\n","            # topv, topi = outputs.topk(1)\n","            # for i in range(5):\n","            #     s = ' '.join([dataset.index2token[idx] for idx in topi[i]])\n","            #     print(topi[i].view(-1))\n","            #     print(s)\n","            #     print()\n","\n","        # Report epoch results.\n","        print(f'Epoch {e}: loss {running_loss}')"]},{"cell_type":"code","execution_count":655,"metadata":{},"outputs":[],"source":["# Length of vocabulary.\n","n_words = len(dataset.index2token)\n","\n","# Decoder.\n","decoder = SentenceRNN(\n","    n_vocab=n_words,\n","    n_hidden=128,\n","    n_layers=1,\n","    dropout=0.,\n","    bidirectional=False,\n",")"]},{"cell_type":"code","execution_count":656,"metadata":{},"outputs":[],"source":["# Set runtime device.\n","device = torch.device('?cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":657,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["898"]},"metadata":{},"execution_count":657}],"source":["len(dataloader)"]},{"cell_type":"code","execution_count":658,"metadata":{"tags":[]},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-658-333ce1196d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptim_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train(decoder,\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-654-a4cdaaf7d9ff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(decoder, loader, epochs, optimizer_decoder, criterion, device, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtarget_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0;31m# Get best prediction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-652-d6c9e0e23d56>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, lens, hidden, cell)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Pass the input feature vector as the first step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0moutput_packed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_embed_packed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;31m# print('output_packed',output_packed.data.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    665\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    666\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Learning parameters.\n","epochs = 12\n","lr = 1e-2\n","\n","# Train the model.\n","# Display training time too.\n","with timecontextprint():\n","    optim_decoder = torch.optim.Adam(decoder.parameters(), lr=lr)\n","    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","    train(decoder,\n","        loader=dataloader,\n","        epochs=epochs,\n","        optimizer_decoder=optim_decoder,\n","        criterion=criterion,\n","        device=device,\n","    )"]},{"source":["## Evaluate"],"cell_type":"markdown","metadata":{"id":"rNCyPzcSy8Ez"}},{"cell_type":"code","execution_count":1329,"metadata":{},"outputs":[],"source":["def evaluate(encoder, decoder, syllables, device='cpu'):\n","    with torch.no_grad():\n","        encoder.eval()\n","        decoder.eval()\n","\n","        encoder.to(device)\n","        decoder.to(device)\n","\n","        # Convert syllables to one-hot.\n","        syllables_oh = torch.nn.functional.one_hot(syllables, num_classes=syl_count)\n","        syllables_oh.to(device)\n","        print('syllables_oh',syllables_oh.size())\n","\n","        # Encode syllables into feature space.\n","        features = encoder(syllables_oh)\n","        print('features',features.size())\n","\n","        # Initialize hidden output.\n","        decoder_hidden = decoder.init_hc(1, device=device)\n","        decoder_cell = decoder.init_hc(1, device=device)\n","\n","        while True:\n","            # Decode.\n","            decoder_input = torch.LongTensor([[dataset.token2index['<sos>']]], device=device)\n","            decoder_input_lens = torch.LongTensor([1])\n","            outputs, out_lens, (decoder_hidden, decoder_cell,) = decoder(features, decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)   \n","\n","            # Build sentences.\n","            print(outputs)\n","            # topv, topi = outputs.topk(1)\n","            # print(topv, topi)\n","            break\n","\n"]},{"cell_type":"code","execution_count":1330,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["syllables_oh torch.Size([2, 169])\nfeatures torch.Size([2, 169])\ntensor([[[ 3.3785, -6.2565, -6.3329,  ..., -7.9525, -3.3959, -5.3591]]])\n"]}],"source":["syllables = torch.LongTensor([7,5])\n","# print(syl_count)\n","evaluate(encoder, decoder, syllables, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}