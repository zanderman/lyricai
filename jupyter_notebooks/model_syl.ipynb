{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"colab":{"name":"model_syl.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python","version":"3.9.2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3bJ6ZZzry8Er"},"source":["# Song Lyric Generation Using Syllable-count with Next-word Prediction\n","\n","To incorporate the notion of line-level syllable count into next-word prediction we define a variant of the RNN for pure next-word prediction which incorporates encoder and decoder sub-models. This adaptation allows the mapping of the syllable-count feature space to the token feature space as used by the original RNN architecture.\n","\n","This architecture was adapted from several sequence modeling and sequence-to-sequence modeling tutorials:\n","- [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","- [Generating Haiku with Deep Learning](https://towardsdatascience.com/generating-haiku-with-deep-learning-dbf5d18b4246)\n","- [Build Your First Text Classification model using PyTorch](https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/)"]},{"cell_type":"code","metadata":{"id":"bQRi6xm5y8Es"},"source":["import os\n","import sys\n","import pickle\n","import torch\n","import torch.nn\n","import torch.optim\n","import torch.utils.data\n","import nltk\n","from nltk.translate.bleu_score import corpus_bleu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"butZqcQOzOHF","executionInfo":{"status":"ok","timestamp":1619640060237,"user_tz":240,"elapsed":23491,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"c0e9c867-f324-4bc9-8e74-3b36b97b8e92"},"source":["\"\"\"Google Drive\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","root_path = '/content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/'\n","dataset_path = './dataset'\n","store_file = os.path.join(root_path,dataset_path,'lyrics.pickle')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"kTBOCijsy8Es","executionInfo":{"status":"ok","timestamp":1619640060238,"user_tz":240,"elapsed":23466,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"0e03eee0-7c0e-4742-9250-ecef7969f2a1"},"source":["\"\"\"Offline Usage\"\"\"\n","# dataset_path = '../../dataset'\n","# store_file = os.path.join(dataset_path,'lyrics.pickle')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Offline Usage'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Cb-sVEkyy8Et"},"source":["## Construct Dataset class\n","\n","We construct a PyTorch dataset class which aids in loading the pre-processed song lyrics.\n","\n","The pre-processed lyrics come with the following attributes:\n","\n","- `index2token`: Maps token integer ID to actual token\n","- `token2index`: Maps token to integer ID\n","- `counts`: Token frequencies\n","- `corpus`: List of tokenized lyrics for each sentence\n","- `vectors`: List of token ID tensors for each sentence"]},{"cell_type":"code","metadata":{"id":"BcdpCHU5y8Et"},"source":["class WorshipLyricDataset(torch.utils.data.Dataset):\n","    \"\"\"Worhip Song dataset from Genius.\n","    \"\"\"\n","\n","    def __init__(self, path: str):\n","\n","        # Load the pre-processed pickle file.\n","        with open(path, 'rb') as fp:\n","            store = pickle.load(fp)\n","        \n","        # Unpack the pickle.\n","        self.index2token = store['index2token']\n","        self.token2index = store['token2index']\n","        self.counts = store['counts']\n","        self.corpus = store['corpus']\n","        self.vectors = [torch.LongTensor(vec) for vec in store['vectors']]\n","        self.syllables = torch.nn.functional.one_hot(torch.LongTensor(store['syllables'])) # One-hot encoded syllabl counts.\n","\n","    def __len__(self):\n","        return len(self.vectors)\n","\n","    def __getitem__(self, idx):\n","        return (self.vectors[idx], self.syllables[idx],)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"2qOn4O89y8Eu"},"source":["# Construct the data object.\n","dataset = WorshipLyricDataset(path=store_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMsL5bC7V3hF"},"source":["# Split data into train/validation set.\n","n_records = len(dataset)\n","train_len = int(.8 * n_records)\n","test_len = int(.2 * n_records)\n","train_subset, test_subset = torch.utils.data.random_split(dataset, [train_len, test_len], generator=torch.Generator().manual_seed(42))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8VO8Xnd1z4_"},"source":["To support training with variable-length sentences we must pad the input sentences on a per-batch basis. To do this in conjunction with a data loader, we define a \"collate\" function which pads the sentences within each batch."]},{"cell_type":"code","metadata":{"id":"IkJxCjVK_aTw"},"source":["def pad_collate(batch):\n","    \"\"\"Pad batches from dataloader.\n","\n","    This allows for more efficient padding,\n","    by only padding within each batch.\n","    \"\"\"\n","    sentences, syllables = zip(*batch)\n","    sen_lens = torch.LongTensor([len(vec) for vec in sentences])\n","    sen_pad = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n","    syllables = torch.stack(syllables) # Convert tuple of tensors to single 2D tensor.\n","    syllables = syllables.reshape(syllables.size(0),1,syllables.size(1)) # Convert to 3D.\n","    return (sen_pad,syllables,sen_lens,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dc1BcftZ2GQb"},"source":["With the dataset and collation function defined we can now construct a `dataloader`, which will allow us to iterate over the lyrics dataset in batches. All training will be done using this loader object."]},{"cell_type":"code","metadata":{"id":"IFMjBBc4y8Ev"},"source":["# Construct data loader.\n","train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2, collate_fn=pad_collate)\n","test_loader = torch.utils.data.DataLoader(test_subset, batch_size=32, shuffle=True, num_workers=2, collate_fn=pad_collate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2XEogOJk_aTw"},"source":["## Model - Syllable-count Next-word Prediction\n","\n","The decoder architecture is identical to the model used in the pure next-word prediction task, called `TokenRNN`.\n","\n","The encoder model, called `EncoderNet`, is a fully-connected network of linear layers with ReLU and PReLU activations respectively, and a final output linear layer that acts as a feature embedding. The dimensions of this feature embedding are the same as the hidden dimension of the decoder LSTM architecture. The notion of line-level syllable count is injected into the sequence generation process by priming the initial hidden state of the decoder. This allows the encoder and decoder to learn in unison the proper syllable-count features and associated next-word predictions."]},{"cell_type":"code","metadata":{"id":"s_233DbaME5F"},"source":["class EncoderNet(torch.nn.Module):\n","    \"\"\"Encodes syllables to sentence-length feature space.\"\"\"\n","    def __init__(self, syl_count: int, n_embed: int):\n","        super().__init__()\n","\n","        # Dense layer sequence.\n","        self.dense = torch.nn.Sequential(\n","            torch.nn.Linear(in_features=syl_count, out_features=syl_count),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(in_features=syl_count, out_features=syl_count),\n","            torch.nn.PReLU(),\n","        )\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Linear(in_features=syl_count, out_features=n_embed)\n","\n","\n","    def forward(self, syllables):\n","        output = self.dense(syllables.float())\n","        embeddings = self.embed(output)\n","        return embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQS_Mz8N_aTx"},"source":["class TokenRNN(torch.nn.Module):\n","    \"\"\"Sequence model for token prediction.\"\"\"\n","    def __init__(self, n_hidden: int, n_vocab: int, n_layers: int, dropout: float = 0., bidirectional: bool = False):\n","        super().__init__()\n","\n","        self.n_hidden = n_hidden\n","        self.n_vocab = n_vocab\n","        self.n_layers = n_layers\n","        self.bidirectional = bidirectional\n","        self.n_dir = 2 if bidirectional else 1\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=n_hidden,\n","        )\n","\n","        # LSTM layer.\n","        self.lstm = torch.nn.LSTM(\n","            input_size=n_hidden,\n","            hidden_size=n_hidden,\n","            num_layers=n_layers,\n","            dropout=dropout,\n","            bidirectional=bidirectional,\n","            batch_first=True,\n","            )\n","\n","        # Word mapping fully-connected layer.\n","        self.fc = torch.nn.Linear(in_features=n_hidden, out_features=n_vocab)\n","    \n","\n","    def forward(self, sentences: torch.Tensor, lens: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor):\n","        \"\"\"\n","        Args:\n","            sentences (torch.Tensor): Sentence word vectors.\n","            lens (torch.Tensor): True lengths of padded sentence vectors.\n","            hidden (torch.Tensor): Hidden state vector.\n","            cell (torch.Tensor): Cell state vector.\n","        \"\"\"\n","        \n","        # Embed the sentence vectors as floating-point.\n","        #\n","        # inputs: (batch_size, sentence_length,)\n","        sentences_embed = self.embed(sentences)\n","        # embedded: (batch_size, sentence_length, embed_dim,)\n","\n","        # Pack the embedding so that the paddings are ignored.\n","        sentences_embed_packed = torch.nn.utils.rnn.pack_padded_sequence(\n","            input=sentences_embed,\n","            lengths=lens, \n","            batch_first=True,\n","            enforce_sorted=False,\n","            )\n","\n","        # Pass the input feature vector as the first step.\n","        output_packed, (hidden, cell) = self.lstm(sentences_embed_packed, (hidden,cell,))\n","\n","        # Get padded output\n","        output_padded, output_lens = torch.nn.utils.rnn.pad_packed_sequence(output_packed, batch_first=True)\n","\n","        # Obtain token-level classification.\n","        output_padded_fc = self.fc(output_padded)\n","\n","        # Run packing on output layer.\n","        return output_padded_fc, output_lens, (hidden, cell,)\n","\n","    def init_hc(self, batch_size: int, device: str = 'cpu') -> torch.Tensor:\n","        \"\"\"Helepr to zero-initialize hidden and cell state tensors.\"\"\"\n","        return torch.zeros((self.n_layers*self.n_dir, batch_size, self.n_hidden), device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_e9ODqNiy8Ew"},"source":["## Train\n","\n","Here we define some helper classes and functions for timing the training rounds."]},{"cell_type":"code","metadata":{"id":"7Fw74eIsy8Ex"},"source":["import time\n","from contextlib import contextmanager\n","\n","class timecontext:\n","    \"\"\"Elapsed time context manager.\"\"\"\n","    def __enter__(self):\n","        self.seconds = time.time()\n","        return self\n","    \n","    def __exit__(self, type, value, traceback):\n","        self.seconds = time.time() - self.seconds\n","\n","@contextmanager\n","def timecontextprint(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    with timecontext() as t:\n","        yield t\n","    print(f\"{description}: {t.seconds} seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rN445XAnnXlg"},"source":["The training itself is done by initializing the LSTM hidden and cell states to zero. Then to generalize all sentence structures we always set the first token to be run through the model as the start-of-sentence (SOS) token. \n","\n","To better generalize the next-token predictions we apply a technique known as \"teacher forcing\". In teacher forcing we pass the known next-token target value at each step as the input to the next decoder step. This forces the decoder to learn using the proper next-token rather than solely based on the predictions at each step. To increase generalization performance further we randomly apply teacher forcing for each batch based on a probability distribution (by default 50% probability). "]},{"cell_type":"code","metadata":{"id":"ib7AXBk3_aTy"},"source":["import random\n","from typing import List, Tuple\n","def train(encoder, decoder, train_loader, test_loader, epochs, optimizer_encoder, optimizer_decoder, criterion, device='cpu', teacher_force_ratio=0.5)  -> Tuple[List[float],List[float]]:\n","    # Send model to device.\n","    encoder.to(device)\n","    decoder.to(device)\n","\n","    # Define loss lists.\n","    train_loss = []\n","    test_loss = []\n","\n","    # Compute batch lengths for averages.\n","    n_train = len(train_loader)\n","    n_test = len(test_loader)\n","\n","    for e in range(epochs):\n","\n","        # Train models.\n","        encoder.train()\n","        decoder.train()\n","        running_loss = 0.0\n","        for b,batch in enumerate(train_loader):\n","            sentences,syllables,sen_lens = batch\n","            sentences = sentences.to(device)\n","            syllables = syllables.to(device)\n","            sen_lens = sen_lens.to(device)\n","\n","            # Encode syllables into feature space.\n","            features = encoder(syllables)\n","            features = features.view(features.size(1), features.size(0), features.size(2))\n","\n","            # Initialize hidden layer to be the features.\n","            decoder_hidden = features # decoder.init_hc(32, device=device)\n","            decoder_cell = features # decoder.init_hc(32, device=device)\n","\n","            # Setup initial decoder inputs.\n","            SOS_token = dataset.token2index['<sos>']\n","            decoder_input = SOS_token*torch.ones((sentences.size(0), 1,), dtype=torch.long, device=device)\n","            decoder_input_lens = torch.ones((sentences.size(0),), dtype=torch.long)\n","\n","            # Initialize batch loss to zero.\n","            loss = 0\n","\n","            # Determine if teacher-forcing should be used for this batch.\n","            use_teacher_forcing = True if random.random() < teacher_force_ratio else False\n","\n","            # Teacher forcing.\n","            # Feed the target as the next input.\n","            if use_teacher_forcing:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # For teacher forcing set the input of the\n","                    # next round to be the current target.\n","                    decoder_input = targets.detach()\n","\n","            # No teacher forcing.\n","            # Feed the RNN predictions as the next input.\n","            else:\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Get best prediction.\n","                    topv, topi = outputs.topk(1)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # Set the input of the next round to be the current prediction.\n","                    decoder_input = topi.squeeze(dim=-1).detach()\n","\n","            # Back-propagate, and step the optimizers.\n","            loss.backward()\n","            optimizer_encoder.step()\n","            optimizer_decoder.step()\n","            \n","            # Zero the gradients\n","            optimizer_encoder.zero_grad()\n","            optimizer_decoder.zero_grad()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","        # Compute average loss.\n","        running_loss /= n_train\n","\n","        # Preserve training loss.\n","        train_loss.append(running_loss)\n","        print(f'[epoch {e}]: train loss {running_loss}')\n","\n","        # ---\n","\n","        # Validate models.\n","        encoder.eval()\n","        decoder.eval()\n","        with torch.no_grad():\n","            running_loss = 0.0\n","            for b,batch in enumerate(test_loader):\n","                sentences,syllables,sen_lens = batch\n","                sentences = sentences.to(device)\n","                syllables = syllables.to(device)\n","                sen_lens = sen_lens.to(device)\n","\n","                # Encode syllables into feature space.\n","                features = encoder(syllables)\n","                features = features.view(features.size(1), features.size(0), features.size(2))\n","\n","                # Initialize hidden layer to be the features.\n","                decoder_hidden = features # decoder.init_hc(32, device=device)\n","                decoder_cell = features # decoder.init_hc(32, device=device)\n","\n","                # Setup initial decoder inputs.\n","                SOS_token = dataset.token2index['<sos>']\n","                decoder_input = SOS_token*torch.ones((sentences.size(0), 1,), dtype=torch.long, device=device)\n","                decoder_input_lens = torch.ones((sentences.size(0),), dtype=torch.long)\n","\n","                # Initialize batch loss to zero.\n","                loss = 0\n","\n","                # Feed the RNN predictions as the next input.\n","                for target_idx in range(1, sentences.size(1)):\n","                    outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                    # Get best prediction.\n","                    topv, topi = outputs.topk(1)\n","\n","                    # Reshape outputs and targets to fit insize the criterion.\n","                    outputs = outputs.squeeze(dim=1)\n","                    targets = sentences[:,target_idx].reshape(sentences.size(0),-1)\n","\n","                    # Calculate batch loss.\n","                    loss += criterion(\n","                        outputs,\n","                        targets.squeeze(dim=1),\n","                    )\n","\n","                    # Set the input of the next round to be the current prediction.\n","                    decoder_input = topi.squeeze(dim=-1).detach()\n","\n","                # Accumulate the loss for this epoch.\n","                running_loss += loss.item()\n","\n","            # Compute average loss.\n","            running_loss /= n_test\n","\n","            # Preserve testing loss.\n","            test_loss.append(running_loss)\n","            print(f'[epoch {e}]: test loss {running_loss}')\n","\n","    # Return list of losses for epochs.\n","    return train_loss, test_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fPRgo--_aTy"},"source":["# Length of vocabulary.\n","n_words = len(dataset.index2token)\n","syl_count = len(dataset.syllables[0])\n","n_hidden = 256\n","\n","# Encoder.\n","encoder = EncoderNet(\n","    syl_count=syl_count,\n","    n_embed=n_hidden,\n",")\n","\n","# Decoder.\n","decoder = TokenRNN(\n","    n_vocab=n_words,\n","    n_hidden=n_hidden,\n","    n_layers=1,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0wo-azdpYoj"},"source":["To speed-up training PyTorch allows us to leverage a GPU, using CUDA, if one is available. Since training a CNN can be computationally intensive we prefer to use a GPU for speed, but will revert to using the CPU if necessary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcoqyU4X_aTy","executionInfo":{"status":"ok","timestamp":1619640066840,"user_tz":240,"elapsed":30021,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"6e02540b-a552-4b3f-ca1b-b1dda6f3850a"},"source":["# Set runtime device.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device: {device}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Device: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qrwgQjIs2kp3"},"source":["With the model defined we can now train it on the lyrics dataset.\n","\n","We define a set of training hyperparameters `epochs` and `lr` which are number of training iterations and optimizer learning rate respectively.\n","\n","For training we use the `Adam` optimizer and `CrossEntropyLoss` criterion since each next-token prediction is essentially a classification task.\n","\n","To speed-up subsequent runs, we also save the trained model to a file. This allows us to train the model once, and then simply load the pre-trained model (using the flag `load_from_file = True`) if the Jupyter notebook is run multiple times during a single session."]},{"cell_type":"code","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"lCfDO1Ko_aTz","executionInfo":{"status":"ok","timestamp":1619640079031,"user_tz":240,"elapsed":42187,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"89e5cac1-175b-46fc-9e26-7fd10984ca87"},"source":["import numpy as np\n","\n","# Define path to decoder model storage.\n","load_from_file = True\n","encoder_store = os.path.join(root_path,dataset_path,'encoder_syl.pt')\n","decoder_store = os.path.join(root_path,dataset_path,'decoder_syl.pt')\n","loss_store = os.path.join(root_path, dataset_path, 'loss_syl.npy')\n","\n","# Load model from store file.\n","if load_from_file and os.path.exists(decoder_store) and os.path.exists(encoder_store) and os.path.exists(loss_store):\n","    print(f'Loading encoder model: {encoder_store}')\n","    print(f'Loading decoder model: {decoder_store}')\n","    decoder.load_state_dict(torch.load(decoder_store))\n","    encoder.load_state_dict(torch.load(encoder_store))\n","\n","    # Read losses from file.\n","    print(f'Loading losses: {loss_store}')\n","    with open(loss_store, 'rb') as fp:\n","       train_loss_rounds, test_loss_rounds = np.load(fp)\n","\n","else:\n","    # Learning parameters.\n","    rounds = 10\n","    epochs = 10\n","    lr = 1e-3\n","    print(f'Training models: rounds={rounds}, epochs={epochs}, lr={lr}, batches={len(train_loader)}')\n","\n","    # Train the model.\n","    # Display training time too.\n","    train_loss_rounds = []\n","    test_loss_rounds = []\n","    with timecontextprint():\n","        optim_encoder = torch.optim.Adam(encoder.parameters(), lr=lr)\n","        optim_decoder = torch.optim.Adam(decoder.parameters(), lr=lr)\n","        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","        for round in range(rounds):\n","            with timecontextprint(f\"[round {round}] elapsed time\"):\n","                train_loss, test_loss = train(encoder, decoder,\n","                    train_loader=train_loader,\n","                    test_loader=test_loader,\n","                    epochs=epochs,\n","                    optimizer_encoder=optim_encoder,\n","                    optimizer_decoder=optim_decoder,\n","                    criterion=criterion,\n","                    device=device,\n","                )\n","                train_loss_rounds.append(train_loss)\n","                test_loss_rounds.append(test_loss)\n","\n","            # Store model state to file.\n","            torch.save(decoder.state_dict(), decoder_store)\n","            torch.save(encoder.state_dict(), encoder_store)\n","            print(f'[round {round}] saved encoder model: {encoder_store}')\n","            print(f'[round {round}] saved decoder model: {decoder_store}')\n","\n","            # Save losses to file too.\n","            with open(loss_store, 'wb') as fp:\n","                np.save(fp, [train_loss_rounds, test_loss_rounds])\n","        \n","    train_loss_rounds = np.array(train_loss_rounds) # Convert to numpy array.\n","    test_loss_rounds = np.array(test_loss_rounds) # Convert to numpy array."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading encoder model: /content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/./dataset/encoder_syl.pt\n","Loading decoder model: /content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/./dataset/decoder_syl.pt\n","Loading losses: /content/gdrive/My Drive/Virginia Tech/graduate/courses/2021_spring/ece_5424/assignments/project_ece_5424/./dataset/loss_syl.npy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"ZVb_sKsZWEOJ","executionInfo":{"status":"ok","timestamp":1619640079269,"user_tz":240,"elapsed":42399,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"2a2ab32d-4a58-4e4d-843c-412bf79a22f7"},"source":["# Plot the losses\n","import matplotlib.pyplot as plt\n","plt.plot(train_loss_rounds.flatten(), label='Train')\n","plt.plot(test_loss_rounds.flatten(), label='Test')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.savefig(os.path.join(root_path,dataset_path,'loss_syl.png'), dpi=150, bbox_inches='tight')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"rNCyPzcSy8Ez"},"source":["## Evaluate\n","\n","Now that the model has been trained we can use it to generate song lyrics.\n","\n","We define a helper function to evaluate the decoder model. The process of decoder evaluation is actually very similar to training. The difference is that for each token prediction we randomly choose the predicted token based on the normalized probability distribution of the prediction set. We also do not employ teacher forcing since the next-token at each step is unknown.\n","\n","The evaluation process continuously loops through next-token predictions until either an end-of-sentence (EOS) token or a maximum decoded token length is reached."]},{"cell_type":"code","metadata":{"id":"QCzrY_eKDi7H"},"source":["import numpy as np\n","from typing import List\n","\n","def evaluate(encoder, decoder, syllables, seed='<sos>', max_length=None, device='cpu') -> List[str]:\n","    \"\"\"Generate a sequence of tokens using the decoder and a starting seed.\"\"\"\n","    encoder.to(device)\n","    encoder.eval()\n","    decoder.to(device)\n","    decoder.eval()\n","    with torch.no_grad():\n","\n","        # Encode syllables into feature space.\n","        syllables = torch.nn.functional.one_hot(torch.tensor([[syllables]], dtype=torch.long, device=device), num_classes=syl_count)\n","        features = encoder(syllables)\n","        features = features.view(features.size(1), features.size(0), features.size(2))\n","\n","        # Initialize hidden layer to be the features.\n","        decoder_hidden = features # decoder.init_hc(32, device=device)\n","        decoder_cell = features # decoder.init_hc(32, device=device)\n","\n","        # Setup initial decoder inputs.\n","        EOS_index = dataset.token2index['<eos>']\n","        SOS_index = dataset.token2index['<sos>']\n","        seed_token = dataset.token2index.get(seed, SOS_index)\n","        decoder_input = seed_token*torch.ones((1, 1,), dtype=torch.long, device=device)\n","        decoder_input_lens = torch.ones((1,), dtype=torch.long)\n","\n","        # Always initialize the deocded tokens with an SOS token.\n","        decoded_tokens = [seed]\n","\n","        # Loop indefinitely until token list is generated.\n","        while True:\n","\n","            # Run current inputs, hidden and cell states through the decoder.\n","            outputs, out_lens, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","            # Normalize the probability distribution of the current prediction.\n","            probs = np.array(torch.nn.functional.softmax(outputs, dim=2).squeeze().cpu())\n","            probs = probs / probs.sum()\n","\n","            # Choose the top prediction from the normalized probability disrtribution above.\n","            topi = torch.tensor(np.random.choice(decoder.n_vocab, 1, p=probs, replace=False), dtype=torch.long, device=device).view(1,1,1)\n","\n","            # Add the current token to the decoded list.\n","            decoded_tokens.append(dataset.index2token[topi.item()])\n","\n","            # Stop if current token is EOS or maximum sentence length has been reached.\n","            if (topi.item() == EOS_index) or (max_length and len(decoded_tokens) == max_length):\n","                break\n","\n","            # Set the input of the next round to be the current prediction.\n","            decoder_input = topi.squeeze(dim=-1).detach()\n","\n","        return decoded_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykEZeefsuvb2"},"source":["We also define a helper function to generate song lyrics, composed of multiple lines, by inputting a set of line-count, optional maximum sentence length, and optional seed list for each sentence. This generates a sequence of tokens for each line of the song and then subsequently joins all lines into a single newline-delimited string."]},{"cell_type":"code","metadata":{"id":"rAi8p8CP_aT0"},"source":["def generate_song(syllables: list, max_length: int = None, seeds: list = None) -> str:\n","    \"\"\"Helper to generate song lyrics given constraints.\"\"\"\n","    num_lines = len(syllables)\n","\n","    # Build list of line seeds if none were provided.\n","    seed = '<sos>'\n","    if not seeds:\n","        seeds = [seed]*num_lines\n","\n","    # Generate predictions for each line.\n","    lines = []\n","    for i in range(num_lines):\n","        tokens = evaluate(encoder, decoder, syllables[i], max_length=max_length, seed=seeds[i], device=device)\n","        if tokens[0] != '<sos>': tokens = ['<sos>'] + tokens\n","        lines.append(' '.join(tokens))\n","\n","    # Join lines together and return as single string.\n","    return '\\n'.join(lines)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omsy3YOj_aT0","executionInfo":{"status":"ok","timestamp":1619640079271,"user_tz":240,"elapsed":42370,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"02793bf5-d5e3-4272-d13d-9951c44b91db"},"source":["print(generate_song(syllables=[10, 5, 7], max_length=10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<sos> lamb was slain for all my sin and shame\n","<sos> one could express <eos>\n","<sos> i salvation is here <eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXuEKFKz_aT1","executionInfo":{"status":"ok","timestamp":1619640079271,"user_tz":240,"elapsed":42344,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"12d3f344-6af3-455b-a234-1a2ad5e4a00d"},"source":["print(generate_song(syllables=[7, 5, 7], seeds=['god', 'we', 'they']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<sos> god all it is done with mine <eos>\n","<sos> we climb it all <eos>\n","<sos> they was blind is in this <eos>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65_gGRFgNZKG","executionInfo":{"status":"ok","timestamp":1619640079271,"user_tz":240,"elapsed":42319,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"de65d112-198e-4ec4-e0c6-b29517b8485d"},"source":["print(generate_song(syllables=[7, 10, 7], seeds=['we', 'give', 'you',]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<sos> we harm will be lost in <eos>\n","<sos> give your holy spirit come and will rise <eos>\n","<sos> you sinners to <unk> <eos>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RDfeY9KeTBJ8"},"source":["Now let's generate a list of 3-line songs and save them to a file."]},{"cell_type":"code","metadata":{"id":"wbO3PT46Ne8O"},"source":["import random\n","\n","n_songs = 10\n","num_lines = 3\n","rng = (5, 11) # Random syllable count range\n","outfile = os.path.join(root_path, dataset_path, 'gen_lyrics_syl.txt')\n","overwrite = False\n","if overwrite or not os.path.exists(outfile):\n","    with open(outfile, 'w') as fp:\n","        for n in range(n_songs):\n","            syllables=[random.randint(*rng) for _ in range(num_lines)]\n","            lyrics = generate_song(syllables=syllables)\n","            fp.write(f\"{syllables}\\n\")\n","            fp.write(lyrics + '\\n\\n')\n","            print(syllables)\n","            print(lyrics + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBHxvVqI6PFJ"},"source":["Now lets check the accuracy of the sentences. To do this we will compute the Bilingual Evaluation Understudy Score (BLEU). The BLEU is a metric for evaluating a generated sentence w.r.t. a refernce sentence."]},{"cell_type":"code","metadata":{"id":"0jmdP0K_9pYY"},"source":["from itertools import zip_longest\n","def grouper(n, iterable, fillvalue=None):\n","    \"\"\"Gleaned from: http://docs.python.org/library/itertools.html#recipes\"\"\"\n","    args = [iter(iterable)]*n\n","    return zip_longest(fillvalue=fillvalue, *args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qN1FVseY6J_V","executionInfo":{"status":"ok","timestamp":1619640105680,"user_tz":240,"elapsed":68708,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"f8dde316-692b-4dec-b2f2-540aec9f29c5"},"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import ast\n","\n","scores = []\n","with open(outfile, 'r') as fp:\n","    lines = [l.strip() for l in fp.readlines() if l.strip()]\n","    for s,tup in enumerate(grouper(4, lines)):\n","        syllables = ast.literal_eval(tup[0])\n","        lyrics = [l.split(' ')[1:] for l in tup[1:]] # Split on whitespace and remove \"<sos>\" token.\n","        for i,l in enumerate(lyrics):\n","            cc = SmoothingFunction()\n","            score = sentence_bleu(dataset.corpus, l, smoothing_function=cc.method4)\n","            scores.append(score)\n","            print(f\"[song={s}, line={i}] BLEU={score}, syllables={syllables[i]}, sentence={l}\")\n","\n","# Compute total BLEU score across all runs.\n","total_score = sum(scores)\n","print(f\"Total score: {total_score} BLEU ({len(scores)} candidates)\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[song=0, line=0] BLEU=1.0, syllables=7, sentence=['if', 'i', 'outrun', 'your', 'grace', '<eos>']\n","[song=0, line=1] BLEU=0.3833076531642739, syllables=8, sentence=['that', 'the', '<unk>', 'is', 'finished', '<eos>']\n","[song=0, line=2] BLEU=0.567358805874344, syllables=8, sentence=['you', 'heaven', 'is', 'love', 'and', '<eos>']\n","[song=1, line=0] BLEU=0.4518010018049224, syllables=8, sentence=['was', 'a', 'dead', 'man', 'that', 'could', '<pad>', '<eos>']\n","[song=1, line=1] BLEU=0.5126665537537463, syllables=7, sentence=['is', 'you', 'in', 'the', 'complete', '<eos>']\n","[song=1, line=2] BLEU=0.4518010018049224, syllables=11, sentence=['your', 'grace', 'you', 'are', 'captured', 'me', 'whole', 'love', '<eos>']\n","[song=2, line=0] BLEU=1.0, syllables=9, sentence=['life', 'is', 'first', 'cry', 'to', 'final', 'breath', '<eos>']\n","[song=2, line=1] BLEU=1.0, syllables=10, sentence=['one', 'and', 'only', 'son', 'to', 'save', 'us', '<eos>']\n","[song=2, line=2] BLEU=1.0, syllables=6, sentence=['will', 'say', 'no', 'other', '<eos>']\n","[song=3, line=0] BLEU=0.5555238068023582, syllables=10, sentence=['now', 'i', 'reach', 'that', 'you', 'are', 'my', 'eyes', '<eos>']\n","[song=3, line=1] BLEU=0.3961546250467316, syllables=11, sentence=['cried', 'be', '<unk>', 'and', 'man', 'and', 'no', 'man', '<eos>']\n","[song=3, line=2] BLEU=0.8408964152537145, syllables=8, sentence=['can', 'not', 'stand', 'in', 'the', 'ground', '<eos>']\n","[song=4, line=0] BLEU=0.5081327481546147, syllables=9, sentence=['my', 'way', 'too', 'real', 'and', 'i', 'know', '<eos>']\n","[song=4, line=1] BLEU=1.0, syllables=6, sentence=['a', 'new', 'creation', '<eos>']\n","[song=4, line=2] BLEU=0.4011832589996506, syllables=7, sentence=['are', 'waking', 'hearts', 'let', 'life', '<eos>']\n","[song=5, line=0] BLEU=0.8408964152537145, syllables=6, sentence=['so', 'faithful', 'to', 'me', '<eos>']\n","[song=5, line=1] BLEU=0.5163449000315277, syllables=11, sentence=['tongue', 'or', 'two', 'take', 'me', 'there', 'is', 'no', '<eos>']\n","[song=5, line=2] BLEU=0.5420788816663185, syllables=10, sentence=['victory', 'forever', 'is', 'the', 'cure', '<eos>']\n","[song=6, line=0] BLEU=0.4120937887172213, syllables=7, sentence=['i', 'sick', 'and', 'move', 'the', '<eos>']\n","[song=6, line=1] BLEU=0.8408964152537145, syllables=6, sentence=['one', 'and', 'have', 'your', 'way', '<eos>']\n","[song=6, line=2] BLEU=1.0, syllables=8, sentence=['have', 'purpose', 'i', 'have', 'meaning', '<eos>']\n","[song=7, line=0] BLEU=0.38440009661674196, syllables=6, sentence=['have', 'got', 'one', '<unk>', '<eos>']\n","[song=7, line=1] BLEU=1.0, syllables=10, sentence=['matter', 'what', 'happens', 'you', 'are', 'holding', '<eos>']\n","[song=7, line=2] BLEU=0.3923372811481588, syllables=10, sentence=['i', 'you', 'when', 'the', 'rhythm', 'it', 'can', 'praise', '<eos>']\n","[song=8, line=0] BLEU=1.0, syllables=7, sentence=['every', 'breath', 'inside', '<eos>']\n","[song=8, line=1] BLEU=0.39380350026831556, syllables=5, sentence=['my', 'and', 'see', 'come', '<eos>']\n","[song=8, line=2] BLEU=0.49682471767288044, syllables=9, sentence=['i', 'are', 'least', 'that', 'you', 'may', 'not', 'see', '<eos>']\n","[song=9, line=0] BLEU=1.0, syllables=9, sentence=['are', 'my', 'reward', 'love', 'unfailing', '<eos>']\n","[song=9, line=1] BLEU=0.5245002946223603, syllables=7, sentence=['how', 'glorious', 'you', '<eos>']\n","[song=9, line=2] BLEU=1.0, syllables=5, sentence=['your', 'nearness', 'lord', 'i', 'wait', '<eos>']\n","Total score: 20.413002161910235 BLEU (30 candidates)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b8ZXGFsAZXX_"},"source":[""],"execution_count":null,"outputs":[]}]}