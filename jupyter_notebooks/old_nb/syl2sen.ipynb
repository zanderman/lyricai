{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"colab":{"name":"model.ipynb","provenance":[]},"language_info":{"name":"python","version":"3.9.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3bJ6ZZzry8Er"},"source":["# Model Building\n","\n","sources:\n","\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://towardsdatascience.com/generating-haiku-with-deep-learning-dbf5d18b4246\n","- https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"]},{"cell_type":"code","metadata":{"id":"bQRi6xm5y8Es","executionInfo":{"status":"ok","timestamp":1617892488018,"user_tz":240,"elapsed":395,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}}},"source":["import os\n","import sys\n","import pickle\n","import torch\n","import torch.nn\n","import torch.optim\n","import torch.utils.data"],"execution_count":1283,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"butZqcQOzOHF","executionInfo":{"status":"ok","timestamp":1617892507139,"user_tz":240,"elapsed":1105,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"a3c3e803-d2f1-48c6-fde8-a99a53fb42a0"},"source":["\"\"\"Google Drive\"\"\"\n","# # Mount Juliet's google drive\n","# from google.colab import drive\n","# drive.mount('/content/gdrive/')\n","# sys_path = '/content/gdrive/My Drive/project_ece_5424/'\n","# sys.path.append(sys_path)\n","# dataset_path = 'dataset'\n","# store_file = os.path.join(sys_path, dataset_path, 'embedding.pickle')"],"execution_count":1284,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Google Drive'"]},"metadata":{},"execution_count":1284}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kTBOCijsy8Es","executionInfo":{"status":"ok","timestamp":1617892397557,"user_tz":240,"elapsed":1947,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"b825ceee-5e5d-4486-a255-62f2c33cff0f"},"source":["\"\"\"Offline Usage\"\"\"\n","dataset_path = '../../dataset'\n","store_file = os.path.join(dataset_path,'lyrics.pickle')"],"execution_count":1285,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cb-sVEkyy8Et"},"source":["## Construct Dataset class"]},{"cell_type":"code","metadata":{"id":"BcdpCHU5y8Et"},"source":["class WorshipLyricDataset(torch.utils.data.Dataset):\n","    \"\"\"Worhip Song dataset from Genius.\n","    \"\"\"\n","\n","    def __init__(self, path: str):\n","\n","        # Load the pre-processed pickle file.\n","        with open(path, 'rb') as fp:\n","            store = pickle.load(fp)\n","        \n","        # Unpack the pickle.\n","        self.index2token = store['index2token']\n","        self.token2index = store['token2index']\n","        self.counts = store['counts']\n","        self.corpus = store['corpus']\n","        self.vectors = [torch.LongTensor(vec) for vec in store['vectors']]\n","        self.syllables = torch.nn.functional.one_hot(torch.LongTensor(store['syllables'])) # One-hot encoded syllabl counts.\n","\n","    def __len__(self):\n","        return len(self.vectors)\n","\n","    def __getitem__(self, idx):\n","        # lyric = {\n","        #     'vector': self.vectors[idx],\n","        #     'syllables': self.syllables[idx],\n","        # }\n","        return (self.vectors[idx], self.syllables[idx],)"],"execution_count":1286,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"2qOn4O89y8Eu","executionInfo":{"status":"error","timestamp":1617892518083,"user_tz":240,"elapsed":502,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"9e8ebb09-3dbe-42c8-859e-82e751dd51b8"},"source":["# Construct the data object.\n","dataset = WorshipLyricDataset(path=store_file)"],"execution_count":1287,"outputs":[]},{"cell_type":"code","execution_count":1288,"metadata":{},"outputs":[],"source":["def pad_collate(batch):\n","    \"\"\"Pad batches from dataloader.\n","\n","    This allows for more efficient padding,\n","    by only padding within each batch.\n","    \"\"\"\n","    sentences, syllables = zip(*batch)\n","    sen_lens = torch.LongTensor([len(vec) for vec in sentences])\n","    sen_pad = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n","    syllables = torch.stack(syllables) # Convert tuple of tensors to single 2D tensor.\n","    syllables = syllables.reshape(syllables.size(0),1,syllables.size(1)) # Convert to 3D.\n","    syllables = syllables.repeat_interleave(sen_pad.size(1), dim=1) # Duplicate syllable count for every word in each sentence.\n","    # print('pad_collate','sen_lens',sen_lens.size())\n","    # print('pad_collate','sen_pad',sen_pad.size())\n","    # print('pad_collate','syllables',syllables.size())\n","    return (sen_pad,syllables,sen_lens,)"]},{"cell_type":"code","metadata":{"id":"IFMjBBc4y8Ev"},"source":["# Construct data loader.\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=pad_collate)"],"execution_count":1289,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XPFty72y8Ev"},"source":["## Construct Model\n","\n","We use an encode/decode architecture, with **encoder** and **decoder** layers, and also add an **attention** layer.\n","\n","This architecture was adapted from the wondeful PyTorch tutorial [\"NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\"](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n","\n","sources:\n","- https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html"]},{"source":["### Encoder\n","The encoder outputs a value for each word in a given input sentence. For each input word, the encoder outputs the value vector and a hidden state, the hidden state is used for the next input word."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":1290,"metadata":{},"outputs":[],"source":["from typing import List, Tuple\n","\n","class SylEncoderNet(torch.nn.Module):\n","    \"\"\"Encodes syllables to sentence-length feature space.\"\"\"\n","    def __init__(self, syl_count: int, n_embed: int):\n","        super().__init__()\n","\n","        # Dense layer sequence.\n","        self.dense = torch.nn.Sequential(\n","            torch.nn.Linear(in_features=syl_count, out_features=syl_count),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(in_features=syl_count, out_features=syl_count),\n","            torch.nn.PReLU(),\n","        )\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Linear(in_features=syl_count, out_features=n_embed)\n","\n","\n","    def forward(self, syllables):\n","        output = self.dense(syllables.float())\n","        embeddings = self.embed(output)\n","        return embeddings"]},{"cell_type":"code","execution_count":1291,"metadata":{},"outputs":[],"source":["class SenDecoderRNN(torch.nn.Module):\n","    def __init__(self, n_embed: int, n_hidden: int, n_vocab: int, n_layers: int, dropout: float = 0., bidirectional: bool = False):\n","        super().__init__()\n","\n","        self.n_embed = n_embed\n","        self.n_hidden = n_hidden\n","        self.n_vocab = n_vocab\n","        self.n_layers = n_layers\n","        self.bidirectional = bidirectional\n","        self.n_dir = 2 if bidirectional else 1\n","\n","        # LSTM layer.\n","        self.lstm = torch.nn.LSTM(\n","            input_size=n_embed,\n","            hidden_size=n_hidden,\n","            num_layers=n_layers,\n","            dropout=dropout,\n","            bidirectional=bidirectional,\n","            batch_first=True,\n","            )\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=n_embed,\n","        )\n","\n","        # Word mapping fully-connected layer.\n","        self.fc = torch.nn.Linear(in_features=n_hidden, out_features=n_vocab)\n","    \n","    def forward(self, features: torch.Tensor, sentences: torch.Tensor, lens: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor):\n","        \"\"\"\n","        Args:\n","            features (torch.Tensor): Embedded syllable features.\n","            sentences (torch.Tensor): Sentence word vectors.\n","            lens (torch.Tensor): True lengths of padded sentence vectors.\n","            hidden (torch.Tensor): Hidden state vector.\n","            cell (torch.Tensor): Cell state vector.\n","        \"\"\"\n","        \n","        # Embed the sentence vectors as floating-point.\n","        #\n","        # inputs: (batch_size, sentence_length,)\n","        sentences_embed = self.embed(sentences)\n","        # embedded: (batch_size, sentence_length, embed_dim,)\n","\n","        # print('features',features.size())\n","        # print('sentences',sentences.size())\n","        # print('lens',lens.size())\n","        # print('hidden',hidden.size())\n","        # print('sentences_embed',sentences_embed.size())\n","        # print('sentences_embed',0,sentences_embed[0][0])\n","\n","        # Pack the embedding so that the paddings are ignored.\n","        sentences_embed_packed = torch.nn.utils.rnn.pack_padded_sequence(\n","            input=sentences_embed,\n","            lengths=lens, \n","            batch_first=True,\n","            enforce_sorted=False,\n","            )\n","        # print('sentences_embed_packed','data',sentences_embed_packed.data.size())\n","\n","        # LSTM layer.\n","        output_packed, (hidden, cell) = self.lstm(sentences_embed_packed, (hidden,cell,))\n","        # print('output_packed','data',output_packed.data.size())\n","        # print('hidden',hidden.size())\n","        # print('cell',cell.size())\n","\n","        # Get padded output\n","        output_padded, output_lens = torch.nn.utils.rnn.pad_packed_sequence(output_packed, batch_first=True)\n","        # print('output_padded',output_padded.size())\n","        # print('output_lens',output_lens.size())\n","\n","        # Obtain word-level classification.\n","        output_padded_fc = self.fc(output_padded)\n","        # print('output_padded_fc',output_padded_fc.size())\n","\n","        # Run packing on output layer.\n","        # return output_padded, output_lens, hidden\n","        return output_padded_fc, output_lens, (hidden, cell,)\n","\n","    def init_hc(self, batch_size: int, device: str = 'cpu'):\n","        return torch.zeros((self.n_layers*self.n_dir, batch_size, self.n_hidden), device=device)"]},{"cell_type":"markdown","metadata":{"id":"_e9ODqNiy8Ew"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"7Fw74eIsy8Ex"},"source":["import time\n","from contextlib import contextmanager\n","@contextmanager\n","def timing(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    start_time = time.time()\n","    yield\n","    stop_time = time.time()\n","    print(f\"{description}: {stop_time - start_time} seconds\")"],"execution_count":1292,"outputs":[]},{"cell_type":"code","execution_count":1293,"metadata":{},"outputs":[],"source":["def train(encoder, decoder, loader, epochs, optimizer_encoder, optimizer_decoder, criterion, device='cpu'):\n","    encoder.to(device)\n","    decoder.to(device)\n","\n","    encoder.train()\n","    decoder.train()\n","\n","    # Initialize hidden output.\n","    decoder_hidden = decoder.init_hc(32, device=device)\n","    decoder_cell = decoder.init_hc(32, device=device)\n","\n","    for e in range(epochs):\n","        running_loss = 0.0\n","        for sentences,syllables,sen_lens in loader:\n","            # print('sentences',sentences.size())\n","            # print('sen_lens',sen_lens.size())\n","            # print('syllables',syllables.size())\n","\n","            # Zero the gradients\n","            optimizer_encoder.zero_grad()\n","            optimizer_decoder.zero_grad()\n","\n","            # Encode syllables into feature space.\n","            features = encoder(syllables)\n","\n","            # Decode.\n","            outputs, out_lens, _ = decoder(features, sentences, sen_lens, decoder_hidden, decoder_cell)\n","\n","            # Calculate batch loss.\n","            # print(outputs.view(-1, decoder.n_vocab).size())\n","            # print(sentences.contiguous().view(-1).size())\n","            loss = criterion(\n","                outputs.view(-1, decoder.n_vocab),\n","                sentences.contiguous().view(-1),\n","            )\n","            # print(f\"[{e}] loss: {loss}\")\n","\n","            # Back-propagate, and step the optimizers.\n","            loss.backward()\n","            optimizer_encoder.step()\n","            optimizer_decoder.step()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","            # topv, topi = outputs.topk(1)\n","            # for i in range(5):\n","            #     s = ' '.join([dataset.index2token[idx] for idx in topi[i]])\n","            #     print(topi[i].view(-1))\n","            #     print(s)\n","            #     print()\n","\n","        # Report epoch results.\n","        print(f'Epoch {e}: loss {running_loss}')"]},{"cell_type":"code","execution_count":1294,"metadata":{},"outputs":[],"source":["# Length of vocabulary.\n","n_words = len(dataset.index2token)\n","syl_count = len(dataset.syllables[0])\n","\n","# Encoder.\n","encoder = SylEncoderNet(\n","    syl_count=syl_count,\n","    n_embed=syl_count,\n",")\n","\n","# Decoder.\n","decoder = SenDecoderRNN(\n","    n_embed=syl_count,\n","    n_hidden=128,\n","    n_vocab=n_words,\n","    n_layers=1,\n","    dropout=0.,\n","    bidirectional=False,\n",")"]},{"cell_type":"code","execution_count":1295,"metadata":{},"outputs":[],"source":["# Set runtime device.\n","device = torch.device('?cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":1296,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: loss 1145.8172444663942\n","Epoch 1: loss 24.32135947328061\n","Epoch 2: loss 7.01588012999855\n","Epoch 3: loss 3.236109237652272\n","Epoch 4: loss 1.7236514538526535\n","Epoch 5: loss 0.9944468085304834\n","Epoch 6: loss 0.6024572528840508\n","Epoch 7: loss 0.3713111925171688\n","Epoch 8: loss 0.2294114887190517\n","Epoch 9: loss 0.14558592369576218\n","Elapsed time: 303.8443510532379 seconds\n"]}],"source":["# Learning parameters.\n","epochs = 10\n","lr = 1e-2\n","\n","# Train the model.\n","# Display training time too.\n","with timing():\n","    optim_encoder = torch.optim.Adam(encoder.parameters(), lr=lr)\n","    optim_decoder = torch.optim.Adam(decoder.parameters(), lr=lr)\n","    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","    train(encoder, decoder,\n","        loader=dataloader,\n","        epochs=epochs,\n","        optimizer_encoder=optim_encoder,\n","        optimizer_decoder=optim_decoder,\n","        criterion=criterion,\n","        device=device,\n","    )"]},{"source":["## Evaluate"],"cell_type":"markdown","metadata":{"id":"rNCyPzcSy8Ez"}},{"cell_type":"code","execution_count":1329,"metadata":{},"outputs":[],"source":["def evaluate(encoder, decoder, syllables, device='cpu'):\n","    with torch.no_grad():\n","        encoder.eval()\n","        decoder.eval()\n","\n","        encoder.to(device)\n","        decoder.to(device)\n","\n","        # Convert syllables to one-hot.\n","        syllables_oh = torch.nn.functional.one_hot(syllables, num_classes=syl_count)\n","        syllables_oh.to(device)\n","        print('syllables_oh',syllables_oh.size())\n","\n","        # Encode syllables into feature space.\n","        features = encoder(syllables_oh)\n","        print('features',features.size())\n","\n","        # Initialize hidden output.\n","        decoder_hidden = decoder.init_hc(1, device=device)\n","        decoder_cell = decoder.init_hc(1, device=device)\n","\n","        while True:\n","            # Decode.\n","            decoder_input = torch.LongTensor([[dataset.token2index['<sos>']]], device=device)\n","            decoder_input_lens = torch.LongTensor([1])\n","            outputs, out_lens, (decoder_hidden, decoder_cell,) = decoder(features, decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)   \n","\n","            # Build sentences.\n","            print(outputs)\n","            # topv, topi = outputs.topk(1)\n","            # print(topv, topi)\n","            break\n","\n"]},{"cell_type":"code","execution_count":1330,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["syllables_oh torch.Size([2, 169])\nfeatures torch.Size([2, 169])\ntensor([[[ 3.3785, -6.2565, -6.3329,  ..., -7.9525, -3.3959, -5.3591]]])\n"]}],"source":["syllables = torch.LongTensor([7,5])\n","# print(syl_count)\n","evaluate(encoder, decoder, syllables, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}