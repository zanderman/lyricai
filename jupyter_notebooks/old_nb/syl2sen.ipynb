{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"colab":{"name":"model.ipynb","provenance":[]},"language_info":{"name":"python","version":"3.9.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3bJ6ZZzry8Er"},"source":["# Model Building\n","\n","sources:\n","\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://towardsdatascience.com/generating-haiku-with-deep-learning-dbf5d18b4246\n","- https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"]},{"cell_type":"code","metadata":{"id":"bQRi6xm5y8Es","executionInfo":{"status":"ok","timestamp":1617892488018,"user_tz":240,"elapsed":395,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}}},"source":["import os\n","import sys\n","import pickle\n","import torch\n","import torch.nn\n","import torch.optim\n","import torch.utils.data"],"execution_count":1371,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"butZqcQOzOHF","executionInfo":{"status":"ok","timestamp":1617892507139,"user_tz":240,"elapsed":1105,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"a3c3e803-d2f1-48c6-fde8-a99a53fb42a0"},"source":["\"\"\"Google Drive\"\"\"\n","# # Mount Juliet's google drive\n","# from google.colab import drive\n","# drive.mount('/content/gdrive/')\n","# sys_path = '/content/gdrive/My Drive/project_ece_5424/'\n","# sys.path.append(sys_path)\n","# dataset_path = 'dataset'\n","# store_file = os.path.join(sys_path, dataset_path, 'embedding.pickle')"],"execution_count":1372,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Google Drive'"]},"metadata":{},"execution_count":1372}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kTBOCijsy8Es","executionInfo":{"status":"ok","timestamp":1617892397557,"user_tz":240,"elapsed":1947,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"b825ceee-5e5d-4486-a255-62f2c33cff0f"},"source":["\"\"\"Offline Usage\"\"\"\n","dataset_path = '../../dataset'\n","store_file = os.path.join(dataset_path,'lyrics.pickle')"],"execution_count":1373,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cb-sVEkyy8Et"},"source":["## Construct Dataset class"]},{"cell_type":"code","metadata":{"id":"BcdpCHU5y8Et"},"source":["class WorshipLyricDataset(torch.utils.data.Dataset):\n","    \"\"\"Worhip Song dataset from Genius.\n","    \"\"\"\n","\n","    def __init__(self, path: str):\n","\n","        # Load the pre-processed pickle file.\n","        with open(path, 'rb') as fp:\n","            store = pickle.load(fp)\n","        \n","        # Unpack the pickle.\n","        self.index2token = store['index2token']\n","        self.token2index = store['token2index']\n","        self.counts = store['counts']\n","        self.corpus = store['corpus']\n","        self.vectors = [torch.LongTensor(vec) for vec in store['vectors']]\n","        self.syllables = torch.nn.functional.one_hot(torch.LongTensor(store['syllables'])) # One-hot encoded syllabl counts.\n","\n","    def __len__(self):\n","        return len(self.vectors)\n","\n","    def __getitem__(self, idx):\n","        # lyric = {\n","        #     'vector': self.vectors[idx],\n","        #     'syllables': self.syllables[idx],\n","        # }\n","        return (self.vectors[idx], self.syllables[idx],)"],"execution_count":1374,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"2qOn4O89y8Eu","executionInfo":{"status":"error","timestamp":1617892518083,"user_tz":240,"elapsed":502,"user":{"displayName":"Juliet Clark","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0ytAuJJH1rCjIXC4Lccol0KV4Aq3F3brKtzYBuy0=s64","userId":"08231115727896360928"}},"outputId":"9e8ebb09-3dbe-42c8-859e-82e751dd51b8"},"source":["# Construct the data object.\n","dataset = WorshipLyricDataset(path=store_file)"],"execution_count":1375,"outputs":[]},{"cell_type":"code","execution_count":1376,"metadata":{},"outputs":[],"source":["def pad_collate(batch):\n","    \"\"\"Pad batches from dataloader.\n","\n","    This allows for more efficient padding,\n","    by only padding within each batch.\n","    \"\"\"\n","    sentences, syllables = zip(*batch)\n","    sen_lens = torch.LongTensor([len(vec) for vec in sentences])\n","    sen_pad = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n","    syllables = torch.stack(syllables) # Convert tuple of tensors to single 2D tensor.\n","    syllables = syllables.reshape(syllables.size(0),1,syllables.size(1)) # Convert to 3D.\n","    syllables = syllables.repeat_interleave(sen_pad.size(1), dim=1) # Duplicate syllable count for every word in each sentence.\n","    # print('pad_collate','sen_lens',sen_lens.size())\n","    # print('pad_collate','sen_pad',sen_pad.size())\n","    # print('pad_collate','syllables',syllables.size())\n","    return (sen_pad,syllables,sen_lens,)"]},{"cell_type":"code","metadata":{"id":"IFMjBBc4y8Ev"},"source":["# Construct data loader.\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=pad_collate)"],"execution_count":1377,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XPFty72y8Ev"},"source":["## Construct Model\n","\n","We use an encode/decode architecture, with **encoder** and **decoder** layers, and also add an **attention** layer.\n","\n","This architecture was adapted from the wondeful PyTorch tutorial [\"NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\"](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n","\n","sources:\n","- https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html"]},{"source":["### Encoder\n","The encoder outputs a value for each word in a given input sentence. For each input word, the encoder outputs the value vector and a hidden state, the hidden state is used for the next input word."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":1378,"metadata":{},"outputs":[],"source":["from typing import List, Tuple\n","\n","class SylEncoderNet(torch.nn.Module):\n","    \"\"\"Encodes syllables to sentence-length feature space.\"\"\"\n","    def __init__(self, syl_count: int, n_embed: int):\n","        super().__init__()\n","\n","        # Dense layer sequence.\n","        self.dense = torch.nn.Sequential(\n","            torch.nn.Linear(in_features=syl_count, out_features=syl_count),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(in_features=syl_count, out_features=syl_count),\n","            torch.nn.PReLU(),\n","        )\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Linear(in_features=syl_count, out_features=n_embed)\n","\n","\n","    def forward(self, syllables):\n","        output = self.dense(syllables.float())\n","        embeddings = self.embed(output)\n","        return embeddings"]},{"cell_type":"code","execution_count":1379,"metadata":{},"outputs":[],"source":["class SenDecoderRNN(torch.nn.Module):\n","    def __init__(self, n_embed: int, n_hidden: int, n_vocab: int, n_layers: int, dropout: float = 0., bidirectional: bool = False):\n","        super().__init__()\n","\n","        self.n_embed = n_embed\n","        self.n_hidden = n_hidden\n","        self.n_vocab = n_vocab\n","        self.n_layers = n_layers\n","        self.bidirectional = bidirectional\n","        self.n_dir = 2 if bidirectional else 1\n","\n","        # LSTM layer.\n","        self.lstm = torch.nn.LSTM(\n","            input_size=n_embed,\n","            hidden_size=n_hidden,\n","            num_layers=n_layers,\n","            dropout=dropout,\n","            bidirectional=bidirectional,\n","            batch_first=True,\n","            )\n","\n","        # Embedding layer.\n","        self.embed = torch.nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=n_embed,\n","        )\n","\n","        # Word mapping fully-connected layer.\n","        self.fc = torch.nn.Linear(in_features=n_hidden, out_features=n_vocab)\n","    \n","    def forward(self, features: torch.Tensor, sentences: torch.Tensor, lens: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor):\n","        \"\"\"\n","        Args:\n","            features (torch.Tensor): Embedded syllable features.\n","            sentences (torch.Tensor): Sentence word vectors.\n","            lens (torch.Tensor): True lengths of padded sentence vectors.\n","            hidden (torch.Tensor): Hidden state vector.\n","            cell (torch.Tensor): Cell state vector.\n","        \"\"\"\n","        \n","        # Embed the sentence vectors as floating-point.\n","        #\n","        # inputs: (batch_size, sentence_length,)\n","        sentences_embed = self.embed(sentences)\n","        # embedded: (batch_size, sentence_length, embed_dim,)\n","\n","        # print('features',features.size())\n","        # print('sentences',sentences.size())\n","        # print('lens',lens.size())\n","        # print('hidden',hidden.size())\n","        # print('sentences_embed',sentences_embed.size())\n","        # print('sentences_embed',0,sentences_embed[0][0])\n","\n","        # Pack the embedding so that the paddings are ignored.\n","        sentences_embed_packed = torch.nn.utils.rnn.pack_padded_sequence(\n","            input=sentences_embed,\n","            lengths=lens, \n","            batch_first=True,\n","            enforce_sorted=False,\n","            )\n","        print('sentences_embed_packed','data',sentences_embed_packed.data.size())\n","\n","        # Pass the input feature vector as the first step.\n","        output_packed, (hidden, cell) = self.lstm(features, (hidden,cell,))\n","\n","        output_packed, (hidden, cell) = self.lstm(sentences_embed_packed, (hidden,cell,))\n","        # print('output_packed','data',output_packed.data.size())\n","        # print('hidden',hidden.size())\n","        # print('cell',cell.size())\n","\n","        # Get padded output\n","        output_padded, output_lens = torch.nn.utils.rnn.pad_packed_sequence(output_packed, batch_first=True)\n","        # print('output_padded',output_padded.size())\n","        # print('output_lens',output_lens.size())\n","\n","        # Obtain word-level classification.\n","        output_padded_fc = self.fc(output_padded)\n","        # print('output_padded_fc',output_padded_fc.size())\n","\n","        # Run packing on output layer.\n","        # return output_padded, output_lens, hidden\n","        return output_padded_fc, output_lens, (hidden, cell,)\n","\n","    def init_hc(self, batch_size: int, device: str = 'cpu'):\n","        return torch.zeros((self.n_layers*self.n_dir, batch_size, self.n_hidden), device=device)"]},{"cell_type":"markdown","metadata":{"id":"_e9ODqNiy8Ew"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"7Fw74eIsy8Ex"},"source":["import time\n","from contextlib import contextmanager\n","@contextmanager\n","def timing(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    start_time = time.time()\n","    yield\n","    stop_time = time.time()\n","    print(f\"{description}: {stop_time - start_time} seconds\")"],"execution_count":1380,"outputs":[]},{"cell_type":"code","execution_count":1381,"metadata":{},"outputs":[],"source":["def train(encoder, decoder, loader, epochs, optimizer_encoder, optimizer_decoder, criterion, device='cpu'):\n","    encoder.to(device)\n","    decoder.to(device)\n","\n","    encoder.train()\n","    decoder.train()\n","\n","    # Initialize hidden output.\n","    decoder_hidden = decoder.init_hc(32, device=device)\n","    decoder_cell = decoder.init_hc(32, device=device)\n","\n","    for e in range(epochs):\n","        running_loss = 0.0\n","        for sentences,syllables,sen_lens in loader:\n","            # print('sentences',sentences.size())\n","            # print('sen_lens',sen_lens.size())\n","            # print('syllables',syllables.size())\n","\n","            # Zero the gradients\n","            optimizer_encoder.zero_grad()\n","            optimizer_decoder.zero_grad()\n","\n","            # Encode syllables into feature space.\n","            features = encoder(syllables)\n","\n","            # Decode.\n","            # outputs, out_lens, _ = decoder(features, sentences, sen_lens, decoder_hidden, decoder_cell)\n","            SOS_token = dataset.token2index['<sos>']\n","            decoder_input = SOS_token*torch.ones((sentences.size(0), sentences.size(1), 1,), dtype=torch.long, device=device)\n","            decoder_input_lens = torch.ones((sentences.size(0),), dtype=torch.long, device=device)\n","\n","            print('decoder_input',decoder_input.size())\n","            print('decoder_input_lens',decoder_input_lens.size())\n","\n","            # Teacher forcing.\n","            # Feed the target as the next input.\n","            loss = 0\n","            for i in range(sentences.size(1)):\n","                outputs, out_lens, _ = decoder(features, decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)\n","\n","                # Calculate batch loss.\n","                # print(outputs.view(-1, decoder.n_vocab).size())\n","                # print(sentences.contiguous().view(-1).size())\n","                loss += criterion(\n","                    outputs.view(-1, decoder.n_vocab),\n","                    sentences.contiguous().view(-1)[i],\n","                )\n","                decoder_input = sentences.contiguous().view(-1)[i]\n","                # loss += criterion(\n","                #     outputs.view(-1, decoder.n_vocab),\n","                #     sentences.contiguous().view(-1),\n","                # )\n","                # print(f\"[{e}] loss: {loss}\")\n","\n","            # Back-propagate, and step the optimizers.\n","            loss.backward()\n","            optimizer_encoder.step()\n","            optimizer_decoder.step()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","            # topv, topi = outputs.topk(1)\n","            # for i in range(5):\n","            #     s = ' '.join([dataset.index2token[idx] for idx in topi[i]])\n","            #     print(topi[i].view(-1))\n","            #     print(s)\n","            #     print()\n","\n","        # Report epoch results.\n","        print(f'Epoch {e}: loss {running_loss}')"]},{"cell_type":"code","execution_count":1382,"metadata":{},"outputs":[],"source":["# Length of vocabulary.\n","n_words = len(dataset.index2token)\n","syl_count = len(dataset.syllables[0])\n","\n","# Encoder.\n","encoder = SylEncoderNet(\n","    syl_count=syl_count,\n","    n_embed=syl_count,\n",")\n","\n","# Decoder.\n","decoder = SenDecoderRNN(\n","    n_embed=syl_count,\n","    n_hidden=128,\n","    n_vocab=n_words,\n","    n_layers=1,\n","    dropout=0.,\n","    bidirectional=False,\n",")"]},{"cell_type":"code","execution_count":1383,"metadata":{},"outputs":[],"source":["# Set runtime device.\n","device = torch.device('?cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":1384,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["decoder_input torch.Size([32, 15, 1])\ndecoder_input_lens torch.Size([32])\nsentences_embed_packed data torch.Size([32, 1, 169])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"input must have 2 dimensions, got 3","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1384-b9da142c7123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptim_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     train(encoder, decoder,\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1381-fe2d4d159161>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, loader, epochs, optimizer_encoder, optimizer_decoder, criterion, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Calculate batch loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1379-da67e315baa5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, sentences, lens, hidden, cell)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0moutput_packed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0moutput_packed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_embed_packed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;31m# print('output_packed','data',output_packed.data.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# print('hidden',hidden.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;31m# See torch/nn/modules/module.py::_forward_unimplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    607\u001b[0m                                'Expected hidden[0] size {}, got {}')\n","\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mexpected_input_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_input_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m    200\u001b[0m                     expected_input_dim, input.dim()))\n","\u001b[0;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 3"]}],"source":["# Learning parameters.\n","epochs = 12\n","lr = 1e-2\n","\n","# Train the model.\n","# Display training time too.\n","with timing():\n","    optim_encoder = torch.optim.Adam(encoder.parameters(), lr=lr)\n","    optim_decoder = torch.optim.Adam(decoder.parameters(), lr=lr)\n","    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","    train(encoder, decoder,\n","        loader=dataloader,\n","        epochs=epochs,\n","        optimizer_encoder=optim_encoder,\n","        optimizer_decoder=optim_decoder,\n","        criterion=criterion,\n","        device=device,\n","    )"]},{"source":["## Evaluate"],"cell_type":"markdown","metadata":{"id":"rNCyPzcSy8Ez"}},{"cell_type":"code","execution_count":1329,"metadata":{},"outputs":[],"source":["def evaluate(encoder, decoder, syllables, device='cpu'):\n","    with torch.no_grad():\n","        encoder.eval()\n","        decoder.eval()\n","\n","        encoder.to(device)\n","        decoder.to(device)\n","\n","        # Convert syllables to one-hot.\n","        syllables_oh = torch.nn.functional.one_hot(syllables, num_classes=syl_count)\n","        syllables_oh.to(device)\n","        print('syllables_oh',syllables_oh.size())\n","\n","        # Encode syllables into feature space.\n","        features = encoder(syllables_oh)\n","        print('features',features.size())\n","\n","        # Initialize hidden output.\n","        decoder_hidden = decoder.init_hc(1, device=device)\n","        decoder_cell = decoder.init_hc(1, device=device)\n","\n","        while True:\n","            # Decode.\n","            decoder_input = torch.LongTensor([[dataset.token2index['<sos>']]], device=device)\n","            decoder_input_lens = torch.LongTensor([1])\n","            outputs, out_lens, (decoder_hidden, decoder_cell,) = decoder(features, decoder_input, decoder_input_lens, decoder_hidden, decoder_cell)   \n","\n","            # Build sentences.\n","            print(outputs)\n","            # topv, topi = outputs.topk(1)\n","            # print(topv, topi)\n","            break\n","\n"]},{"cell_type":"code","execution_count":1330,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["syllables_oh torch.Size([2, 169])\nfeatures torch.Size([2, 169])\ntensor([[[ 3.3785, -6.2565, -6.3329,  ..., -7.9525, -3.3959, -5.3591]]])\n"]}],"source":["syllables = torch.LongTensor([7,5])\n","# print(syl_count)\n","evaluate(encoder, decoder, syllables, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}