{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd064d019df8f386dcc423f9fd084afce6ddb1bf16f9fb1fd7275a3007e4feb955b",
   "display_name": "Python 3.9.2 64-bit ('lyricai-GYvo4rtv': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "source": [
    "## Load lyric files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_files(root_dir: str) -> dict:\n",
    "    return {int(os.path.splitext(os.path.basename(f))[0]): os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.json')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(file_dict: dict):\n",
    "    song_dict = {}\n",
    "    for song_id, path in file_dict.items():\n",
    "        with open(path, 'r') as fp:\n",
    "            song_dict[song_id] = json.load(fp)\n",
    "    return song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "songid_to_file = get_song_files(os.path.join(dataset_path,'songs'))\n",
    "songid_to_song = load_songs(songid_to_file)"
   ]
  },
  {
   "source": [
    "## Cleanse lyrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def decontracted(phrase: str):\n",
    "    \"\"\"Remove English word contractions.\n",
    "\n",
    "    Gleaned from: https://stackoverflow.com/a/47091490\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"wanna\", \"want to\", phrase)\n",
    "    phrase = re.sub(r\"gotta\", \"got to\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def clean_lyric(lyric: str, preserve_lines: bool = False):\n",
    "    lyric = lyric.lower() # Convert to common case.\n",
    "    lyric = re.sub(r'\\[[^\\]]*\\]', '', lyric) # Remove paranthetical content \"[*]\", like markers for chorus and verses.\n",
    "    lyric = re.sub(r'\\([^\\)]*\\)', '', lyric) # Remove paranthetical content \"(*)\", like markers for chorus and verses.\n",
    "    lyric = lyric.strip() # Remove any extra newlines at the ends.\n",
    "    if preserve_lines:\n",
    "        lyric = re.sub(r\"(?:\\s*\\n\\s*)+\", r'\\n', lyric)\n",
    "        lyric = re.sub('\\n', ' NEWLINE ', lyric)\n",
    "    lyric = decontracted(lyric) # Remove contractions before tokenizer to handle special cases.\n",
    "    tokens = word_tokenize(lyric) # Split into word tokens.\n",
    "    tokens = [word for word in tokens if word.isalpha()] # Careful to remove punct after contractions.\n",
    "    if preserve_lines:\n",
    "        tokens = ['\\n' if 'NEWLINE' in word else word for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "songid_to_lyrics = {songid: clean_lyric(song['lyrics'], preserve_lines=True) for songid,song in songid_to_song.items()}"
   ]
  },
  {
   "source": [
    "## Build vocabulary list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique set of words.\n",
    "corpus = sorted(set(sum([lyrics for songid,lyrics in songid_to_lyrics.items()], [])))"
   ]
  },
  {
   "source": [
    "## Create integer mapping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mappings for: int <--> token\n",
    "int_to_token = {i: token for i,token in enumerate(corpus)}\n",
    "token_to_int = {token: i for i,token in int_to_token.items()}"
   ]
  },
  {
   "source": [
    "## "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aah',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abatido',\n",
       " 'abba',\n",
       " 'abide',\n",
       " 'abideth']"
      ]
     },
     "metadata": {},
     "execution_count": 359
    }
   ],
   "source": [
    "len(corpus)\n",
    "corpus[:10]"
   ]
  },
  {
   "source": [
    "## Embed lyrics as integers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "songid_to_embed = {songid: [token_to_int[token] for token in lyrics] for songid,lyrics in songid_to_lyrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['you', 'call', 'me', 'out', 'upon', 'the', 'waters', '\\n', 'the', 'great']\n[6232, 714, 3231, 3645, 5768, 5368, 5951, 0, 5368, 2320]\n"
     ]
    }
   ],
   "source": [
    "print(songid_to_lyrics[147168][:10])\n",
    "print(songid_to_embed[147168][:10])"
   ]
  },
  {
   "source": [
    "## Write embedding to pickle file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "store = {\n",
    "    'mapping': int_to_token,\n",
    "    'embedding': songid_to_embed,\n",
    "}\n",
    "store_file = os.path.join(dataset_path,'embedding.pickle')\n",
    "with open(store_file, 'wb') as fp:\n",
    "    pickle.dump(store, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "# Model Building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "source": [
    "## Construct Dataset class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorshipLyricDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Worhip Song dataset from Genius.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_file: str, sentence_length: int = None, sentence_step: int = 1):\n",
    "        self.sentence_step = sentence_step\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embedding_file = embedding_file\n",
    "\n",
    "        # Load the embedding.\n",
    "        with open(embedding_file, 'rb') as fp:\n",
    "            store = pickle.load(fp)\n",
    "        \n",
    "        self.corpus = store['mapping']\n",
    "        # self.songid_to_embed_all = store['embedding']\n",
    "        # self.idx_to_songid = {idx: songid for idx,songid in enumerate(sorted(store['embedding'].keys()))}\n",
    "        self.songids = sorted(store['embedding'].keys())\n",
    "\n",
    "        # Break each lyric into contiguous sentences.\n",
    "        self.songid_to_embed = {}\n",
    "        self.songid_to_nextword = {}\n",
    "        for songid,embed in store['embedding'].items():\n",
    "\n",
    "            # Group embedding to contiguous sentence length.\n",
    "            if sentence_length:\n",
    "                self.songid_to_embed[songid] = []\n",
    "                self.songid_to_nextword[songid] = []\n",
    "                for i in range(0, len(embed) - self.sentence_length, self.sentence_step):\n",
    "                    self.songid_to_embed[songid].append(embed[i:i+self.sentence_length])\n",
    "                    self.songid_to_nextword[songid].append(embed[i+self.sentence_length])\n",
    "                self.songid_to_embed[songid] = torch.tensor(self.songid_to_embed[songid], dtype=torch.long)\n",
    "                self.songid_to_nextword[songid] = torch.tensor(self.songid_to_nextword[songid], dtype=torch.long)\n",
    "\n",
    "            # Return original lyric embedding.\n",
    "            else:\n",
    "                self.songid_to_embed[songid] = torch.tensor(embed, dtype=torch.long)\n",
    "                self.songid_to_nextword[songid] = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "        # Add padding based on longest song.\n",
    "        self.embed_pad = torch.nn.utils.rnn.pad_sequence([self.songid_to_embed[songid] for songid in self.songids], batch_first=True, padding_value=0)\n",
    "        self.nextword_pad = torch.nn.utils.rnn.pad_sequence([self.songid_to_nextword[songid] for songid in self.songids], batch_first=True, padding_value=0)\n",
    "        # longest_size = max(embed.shape[0] for songid,embed in self.songid_to_embed)\n",
    "        # for songid in self.songid_to_embed.keys():\n",
    "        #     self.songid_to_embed.\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.songid_to_embed)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # songid = self.songids[idx] # Get song ID from dataset index.\n",
    "\n",
    "        lyric = {\n",
    "            # 'embed': self.songid_to_embed[songid],\n",
    "            'embed': self.embed_pad[idx],\n",
    "            # 'nextword': self.songid_to_nextword[songid],\n",
    "            'nextword': self.nextword_pad[idx],\n",
    "            # 'songid': songid,\n",
    "            'songid': self.songids[idx],\n",
    "        }\n",
    "        return lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lyric_dataset = WorshipLyricDataset(embedding_file=store_file, sentence_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1284, 5]), torch.Size([1284]))"
      ]
     },
     "metadata": {},
     "execution_count": 366
    }
   ],
   "source": [
    "lyric_dataset[0]['embed'].shape, lyric_dataset[0]['nextword'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct data loader.\n",
    "dataloader = torch.utils.data.DataLoader(lyric_dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "source": [
    "## Construct Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricGenerator(torch.nn.Module):\n",
    "    def __init__(self, sentence_length: int, corpus_length: int, n_hidden: int = 128, n_layers: int = 2, drop_prob: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=sentence_length,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            dropout=drop_prob,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            )\n",
    "        self.dropout = torch.nn.Dropout(p=drop_prob)\n",
    "        self.fc = torch.nn.Linear(in_features=n_hidden, out_features=corpus_length)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.float() # Convert input to float type for LSTM.\n",
    "\n",
    "        # Run inputs through LSTM.\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        print(lstm_out.shape)\n",
    "\n",
    "        # Pass LSTM outputs through dropout layer.\n",
    "        out = self.dropout(lstm_out)\n",
    "\n",
    "        # Stack-up LSTM outputs.\n",
    "        # out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = out.view(-1, self.n_hidden)\n",
    "        print(out.shape)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "source": [
    "## Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optim, criterion, loader, device='cpu'):\n",
    "    \"\"\"Helper to train the model.\"\"\"\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        running_loss = 0.0\n",
    "        for lyric in loader:\n",
    "\n",
    "            # Send data to desired device.\n",
    "            x = lyric['embed'].to(device)\n",
    "            y = lyric['nextword'].to(device)\n",
    "\n",
    "            # Evaluate the model.\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # Compute losses.\n",
    "            print(y_pred.shape, y.shape)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            # Zero the gradient, back-propagate, and step the optimizer.\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Accumulate the loss for this epoch.\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Report epoch results.\n",
    "        print(f'Epoch {e}: loss {running_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timing(description='Elapsed time'):\n",
    "    \"\"\"Context manager to print elapsed time from call.\"\"\"\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    stop_time = time.time()\n",
    "    print(f\"{description}: {stop_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set runtime device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the model.\n",
    "corpus_length = len(lyric_dataset.corpus)\n",
    "model = LyricGenerator(\n",
    "    sentence_length=lyric_dataset.sentence_length,\n",
    "    corpus_length=corpus_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6272\n"
     ]
    }
   ],
   "source": [
    "print(corpus_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1284, 256])\ntorch.Size([2568, 128])\ntorch.Size([2568, 6272]) torch.Size([1, 1284])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2568) to match target batch_size (1).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-418-e2e554a5f122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-410-a51487006e26>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch, optim, criterion, loader, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Compute losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Zero the gradient, back-propagate, and step the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/lyricai-GYvo4rtv/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2385\u001b[0m             \u001b[0;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2568) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "# Learning parameters.\n",
    "epoch = 1\n",
    "lr = 1e-2\n",
    "\n",
    "# Train the model.\n",
    "# Display training time too.\n",
    "with timing():\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    train(model, loader=dataloader, epoch=epoch, optim=optim, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}