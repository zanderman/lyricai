{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd064d019df8f386dcc423f9fd084afce6ddb1bf16f9fb1fd7275a3007e4feb955b",
   "display_name": "Python 3.9.2 64-bit ('lyricai-GYvo4rtv': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "source": [
    "## Load lyric files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_files(root_dir: str) -> dict:\n",
    "    return {int(os.path.splitext(os.path.basename(f))[0]): os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.json')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(file_dict: dict):\n",
    "    song_dict = {}\n",
    "    for song_id, path in file_dict.items():\n",
    "        with open(path, 'r') as fp:\n",
    "            song_dict[song_id] = json.load(fp)\n",
    "    return song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "songid_to_file = get_song_files(os.path.join(dataset_path,'songs'))\n",
    "songid_to_song = load_songs(songid_to_file)"
   ]
  },
  {
   "source": [
    "## Cleanse lyrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "from typing import List\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus restrictions.\n",
    "min_count = 3 # Minimum word count.\n",
    "min_len = 3 # Minimum sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tokens that have reserved corpus index.\n",
    "PAD_token = '<pad>'\n",
    "SOS_token = '<sos>'\n",
    "EOS_token = '<eos>'\n",
    "UNK_token = '<unk>'\n",
    "reserved_tokens = [PAD_token, SOS_token, EOS_token, UNK_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase: str):\n",
    "    \"\"\"Remove English word contractions.\n",
    "\n",
    "    Gleaned from: https://stackoverflow.com/a/47091490\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"wanna\", \"want to\", phrase)\n",
    "    phrase = re.sub(r\"gotta\", \"got to\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def clean_lyrics(lyric: str) -> List[List[str]]:\n",
    "    \"\"\"Converts a lyric string into a list of sentence tokens.\n",
    "\n",
    "    Sentences are distinguished by both newline ('\\n') and period ('.') separators.\n",
    "    \"\"\"\n",
    "    lyric = lyric.lower() # Convert to common case.\n",
    "    lyric = re.sub(r'\\[[^\\]]*\\]', '', lyric) # Remove paranthetical content \"[*]\", like markers for chorus and verses.\n",
    "    lyric = re.sub(r'\\([^\\)]*\\)', '', lyric) # Remove paranthetical content \"(*)\", like markers for chorus and verses.\n",
    "    lyric = lyric.strip() # Remove any extra newlines at the ends.\n",
    "    lyric = decontracted(lyric) # Remove contractions before tokenizer to handle special cases.\n",
    "\n",
    "    # Replace any periods with newlines to ensure sentences end with a newline.\n",
    "    lyric = re.sub(r\"\\.\", r'\\n', lyric)\n",
    "\n",
    "    # Preserve line structure because tokenizer will remove traditional newlines.\n",
    "    lyric = re.sub(r\"(?:\\s*\\n\\s*)+\", r'\\n', lyric) # Remove repeated newlines.\n",
    "    lyric = re.sub('\\n', ' NEWLINE ', lyric) # Re-map newlines so that tokenizer doesn't remove them.\n",
    "\n",
    "    # Tokenize the entire song into list of words.\n",
    "    tokens = nltk.tokenize.word_tokenize(lyric) # Split into word tokens.\n",
    "    tokens = [word for word in tokens if word.isalpha()] # Careful to remove punct after contractions.\n",
    "\n",
    "    # Group sentences together by line.\n",
    "    tokens_lines = list(filter(None, iter(list(group) for key,group in itertools.groupby(tokens, lambda s: s == 'NEWLINE') if not key)))\n",
    "    return tokens_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the lyrics of each song and combine into 2D list of sentences and tokens.\n",
    "corpus = list(itertools.chain.from_iterable(clean_lyrics(song['lyrics']) for _,song in songid_to_song.items()))\n",
    "\n",
    "# Remove sentence duplicates.\n",
    "corpus = list(map(str.split, set(map(' '.join, corpus))))\n",
    "\n",
    "# Sort corpus by decreasing length.\n",
    "corpus.sort(key=len, reverse=True)\n",
    "\n",
    "# Remove any lines that are below the threshold.\n",
    "if min_len:\n",
    "    corpus = list(itertools.takewhile(lambda sen: len(sen) >= min_len, corpus))\n",
    "\n",
    "# Add EOS token to the end of each sentence.\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i].append(EOS_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counts for each word.\n",
    "counts = collections.Counter(itertools.chain.from_iterable(corpus))\n",
    "\n",
    "# Remove any counts below the threshold.\n",
    "if min_count:\n",
    "    for key,_ in itertools.dropwhile(lambda tup: tup[1] >= min_count, counts.most_common()):\n",
    "        del counts[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of unique words.\n",
    "index2token = reserved_tokens + sorted(set(counts))\n",
    "\n",
    "# Build mappings for: token --> index\n",
    "token2index = {token: i for i,token in enumerate(index2token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate index vectors based on word assignment.\n",
    "# Since some words may have been removed, replace all unknown with the UNK token.\n",
    "vectors = [[token2index.get(token, token2index[UNK_token]) for token in line] for line in corpus]"
   ]
  },
  {
   "source": [
    "### Print some vocabulary statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1339 songs\n28735 unique lines\n2779 unique words\nLongest sentence: 132 words\nShortest sentence: 4 words\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(songid_to_song)} songs\")\n",
    "print(f\"{len(corpus)} unique lines\")\n",
    "print(f\"{len(index2token)} unique words\")\n",
    "print(f\"Longest sentence: {len(corpus[0])} words\")\n",
    "print(f\"Shortest sentence: {len(corpus[-1])} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Line:      ['lord', 'god', 'almighty', '<eos>']\nEmbedding: [1393, 1006, 58, 4]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Line:      {corpus[-1]}\")\n",
    "print(f\"Embedding: {vectors[-1]}\")"
   ]
  },
  {
   "source": [
    "## Collect phonemes\n",
    "\n",
    "Use the CMU Pronouncing Dictionary (cmudict) to collect phonemes for each word.\n",
    "- http://www.speech.cs.cmu.edu/cgi-bin/cmudict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CMU dictionary.\n",
    "cd = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word: str):\n",
    "    \"\"\"Determine the number of syllables in the given word.\n",
    "\n",
    "    This function requires the NLTK cmudict.\n",
    "    \"\"\"\n",
    "    return max(iter(sum(re.match(r'.*\\d', p) is not None for p in phonemes) for phonemes in cd.get(word, [])), default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute syllable counts for each sentence in the corpus.\n",
    "syllables = [sum(count_syllables(token) for token in line) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5: ['lord', 'god', 'almighty', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# Print an example from the corpus.\n",
    "print(f\"{syllables[-1]}: {corpus[-1]}\")"
   ]
  },
  {
   "source": [
    "## Write embedding to pickle file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lyrics vocabulary saved: ../../dataset/lyrics.pickle\n"
     ]
    }
   ],
   "source": [
    "# Path to output pickle file.\n",
    "path = os.path.join(dataset_path, 'lyrics.pickle')\n",
    "\n",
    "# Contents of pickle will be the following dictionary.\n",
    "store = {\n",
    "    'index2token': index2token,\n",
    "    'token2index': token2index,\n",
    "    'vectors': vectors,\n",
    "    'counts': counts,\n",
    "    'corpus': corpus,\n",
    "    'syllables': syllables,\n",
    "}\n",
    "with open(path, 'wb') as fp:\n",
    "    pickle.dump(store, fp, protocol=pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "print(f'Lyrics vocabulary saved: {path}')"
   ]
  }
 ]
}